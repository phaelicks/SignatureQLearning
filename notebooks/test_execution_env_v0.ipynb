{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run base.ipynb\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import abides_gym\n",
    "from policies_v1 import SigPolicy\n",
    "from train_execution_v0 import train\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register market making env for gym use \n",
    "from gym.envs.registration import register\n",
    "from abides_gym.envs.markets_execution_environment_v0 import (\n",
    "    SubGymMarketsExecutionEnv_v0,\n",
    ")\n",
    "\n",
    "register(\n",
    "    id=\"market-execution-v0\",\n",
    "    entry_point=SubGymMarketsExecutionEnv_v0,\n",
    ")\n",
    "\n",
    "def generate_env(seed):\n",
    "    \"\"\"\n",
    "    generates specific environment with the parameters defined and set the seed\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "            \"market-execution-v0\",\n",
    "            background_config=\"rmsc04\",\n",
    "            execution_window=\"04:00:00\",\n",
    "            timestep_duration=\"10s\",\n",
    "            direction=\"BUY\",\n",
    "            parent_order_size=20000,\n",
    "            order_fixed_size=50,\n",
    "            first_interval=\"00:05:00\",\n",
    "            not_enough_reward_update=-100,#penalty\n",
    "            debug_mode=True\n",
    "        )\n",
    "    env.seed(seed)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "env = generate_env(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      " Episode 0 | step 100 | reward 2.5825 | loss 0.34064096794992715\n",
      "Q values: tensor([0.0669, 0.0308, 0.0533], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 200 | reward 4.334075000000001 | loss 0.6410367162229849\n",
      "Q values: tensor([0.1427, 0.1177, 0.1427], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 300 | reward 1.0460750000000005 | loss 0.7976903703030447\n",
      "Q values: tensor([0.1117, 0.1048, 0.1143], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 400 | reward 5.803649999999999 | loss 1.3770542839571507\n",
      "Q values: tensor([0.3270, 0.3338, 0.3261], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 500 | reward 0.7661999999999975 | loss 1.7261111587963855\n",
      "Q values: tensor([0.2415, 0.3297, 0.3644], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 600 | reward 2.3565499999999977 | loss 2.7492695319502474\n",
      "Q values: tensor([0.4947, 0.5491, 0.4873], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 700 | reward 11.708099999999996 | loss 4.163599813467258\n",
      "Q values: tensor([1.0447, 1.0010, 0.9533], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 778 | reward 3.449074999999993 | loss 8.32523429632839\n",
      "Q values: tensor([0.9012, 0.9935, 1.1229], grad_fn=<SelectBackward>)\n",
      "  1%|          | 1/100 [00:48<1:19:15, 48.03s/it]\n",
      " Episode 1 | step 100 | reward 0.824175 | loss 0.2527952203710129\n",
      "Q values: tensor([0.1914, 0.2375, 0.2627], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 200 | reward 9.723374999999997 | loss 1.8977419046647128\n",
      "Q values: tensor([0.4154, 0.3949, 0.4115], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 300 | reward 13.284199999999998 | loss 2.3046514101393365\n",
      "Q values: tensor([0.5035, 0.4730, 0.4868], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 400 | reward 32.354875 | loss 9.79456385465775\n",
      "Q values: tensor([0.9053, 0.8012, 0.8056], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 500 | reward 57.922174999999974 | loss 23.86202530720831\n",
      "Q values: tensor([1.5008, 1.2919, 1.2988], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 600 | reward 86.73122500000002 | loss 43.312317141979946\n",
      "Q values: tensor([2.1860, 1.8587, 1.9605], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 700 | reward 101.40109999999996 | loss 50.768110407029525\n",
      "Q values: tensor([2.8446, 2.5684, 2.7163], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 737 | reward 107.86119999999995 | loss 64.63882394223103\n",
      "Q values: tensor([3.2566, 3.0446, 3.0731], grad_fn=<SelectBackward>)\n",
      "  2%|▏         | 2/100 [01:32<1:15:23, 46.16s/it]\n",
      " Episode 2 | step 100 | reward 3.6312500000000005 | loss 0.36998903124797167\n",
      "Q values: tensor([0.9159, 0.9217, 0.8699], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 200 | reward 10.539599999999998 | loss 1.0979198580523963\n",
      "Q values: tensor([1.1078, 1.0649, 1.0588], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 300 | reward 23.055875 | loss 3.8807862639062343\n",
      "Q values: tensor([1.4137, 1.3582, 1.3390], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 400 | reward 36.51517500000003 | loss 7.497231888925203\n",
      "Q values: tensor([1.7001, 1.6548, 1.6329], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 500 | reward 41.42987500000001 | loss 8.368906824007727\n",
      "Q values: tensor([2.0500, 1.9845, 1.9908], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 600 | reward 46.01477500000001 | loss 8.671411421059904\n",
      "Q values: tensor([2.4773, 2.4049, 2.4162], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 700 | reward 60.790149999999976 | loss 11.356289622287111\n",
      "Q values: tensor([3.1163, 3.0675, 3.0047], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 767 | reward 69.89075000000001 | loss 24.92395646896225\n",
      "Q values: tensor([3.5542, 3.4524, 3.4107], grad_fn=<SelectBackward>)\n",
      "  3%|▎         | 3/100 [02:20<1:15:52, 46.94s/it]\n",
      " Episode 3 | step 100 | reward -1.5999000000000008 | loss 0.3958689176869399\n",
      "Q values: tensor([1.1040, 1.1071, 1.0825], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 200 | reward -0.0038250000000006334 | loss 0.6879945524825786\n",
      "Q values: tensor([1.3792, 1.3637, 1.3472], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 300 | reward 11.204524999999999 | loss 2.9292021893074036\n",
      "Q values: tensor([1.7624, 1.7335, 1.6996], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 400 | reward 34.66512499999999 | loss 11.621029018088535\n",
      "Q values: tensor([2.3175, 2.2347, 2.1759], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 500 | reward 57.11560000000001 | loss 19.974932610246213\n",
      "Q values: tensor([2.9576, 2.8234, 2.7160], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 600 | reward 68.76765 | loss 21.713406226031765\n",
      "Q values: tensor([3.6568, 3.5566, 3.5243], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 700 | reward 71.72112500000009 | loss 22.128428701885895\n",
      "Q values: tensor([4.2129, 4.1676, 4.1440], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 777 | reward 69.58922500000008 | loss 45.913558368141565\n",
      "Q values: tensor([4.5794, 4.5252, 4.4773], grad_fn=<SelectBackward>)\n",
      "  4%|▍         | 4/100 [03:10<1:16:56, 48.09s/it]\n",
      " Episode 4 | step 100 | reward 1.9797499999999995 | loss 0.260036491487881\n",
      "Q values: tensor([0.9730, 0.9393, 0.9356], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 200 | reward -0.5131749999999999 | loss 0.6801675979986754\n",
      "Q values: tensor([1.1038, 1.0700, 1.0916], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 300 | reward -4.2402750000000005 | loss 1.2557863434498415\n",
      "Q values: tensor([1.3424, 1.3153, 1.3392], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 400 | reward -4.133700000000001 | loss 1.3848325774303873\n",
      "Q values: tensor([1.6092, 1.5935, 1.5635], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 500 | reward -13.735925000000005 | loss 4.337097568334329\n",
      "Q values: tensor([1.7966, 1.7553, 1.7881], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 600 | reward -26.831375 | loss 9.32077193674182\n",
      "Q values: tensor([1.9080, 1.8803, 1.8886], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 700 | reward -53.433099999999996 | loss 21.385802527107316\n",
      "Q values: tensor([1.8669, 1.9176, 2.0104], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 769 | reward -72.318225 | loss 34.32404013668189\n",
      "Q values: tensor([1.7836, 1.8505, 1.9637], grad_fn=<SelectBackward>)\n",
      "  5%|▌         | 5/100 [03:55<1:14:31, 47.06s/it]\n",
      " Episode 5 | step 100 | reward -1.3461499999999995 | loss 0.508221670302305\n",
      "Q values: tensor([1.1142, 1.1377, 1.1062], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 200 | reward -6.802999999999999 | loss 1.8095329888381002\n",
      "Q values: tensor([1.2737, 1.3023, 1.2994], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 300 | reward -25.343550000000004 | loss 7.765351652994241\n",
      "Q values: tensor([1.4978, 1.5741, 1.6178], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 400 | reward -47.6242 | loss 12.98368250773342\n",
      "Q values: tensor([1.6993, 1.8396, 1.9401], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 500 | reward -67.22237499999997 | loss 15.371177243882926\n",
      "Q values: tensor([1.9302, 2.0614, 2.2383], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 600 | reward -79.88792500000002 | loss 16.289518961347426\n",
      "Q values: tensor([2.4713, 2.5602, 2.6984], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 700 | reward -103.90505 | loss 21.90107024850579\n",
      "Q values: tensor([2.5740, 2.6999, 2.8980], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 746 | reward -115.71907499999999 | loss 32.44298466079192\n",
      "Q values: tensor([2.2644, 2.3195, 2.6184], grad_fn=<SelectBackward>)\n",
      "  6%|▌         | 6/100 [04:40<1:12:32, 46.30s/it]\n",
      " Episode 6 | step 100 | reward 1.779675 | loss 0.5657809437039703\n",
      "Q values: tensor([0.7263, 0.8035, 0.8187], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 200 | reward 9.604924999999998 | loss 2.6970567437394095\n",
      "Q values: tensor([0.9191, 0.9756, 0.9813], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 300 | reward 24.312524999999997 | loss 7.189563011285969\n",
      "Q values: tensor([1.1395, 1.1502, 1.1474], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 400 | reward 29.78892500000001 | loss 8.774777958225428\n",
      "Q values: tensor([1.3405, 1.3580, 1.3135], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 500 | reward 19.833150000000018 | loss 11.111863170654402\n",
      "Q values: tensor([1.5088, 1.5264, 1.5787], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 600 | reward 6.268975000000001 | loss 14.44261934609213\n",
      "Q values: tensor([1.7149, 1.7652, 1.8514], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 700 | reward -11.222924999999998 | loss 17.111584748384217\n",
      "Q values: tensor([1.8492, 1.8835, 2.0621], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 800 | reward -29.527549999999987 | loss 20.509125313193096\n",
      "Q values: tensor([1.7787, 1.8348, 2.0393], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 805 | reward -31.422549999999987 | loss 26.57927396770802\n",
      "Q values: tensor([1.7510, 1.7973, 2.0116], grad_fn=<SelectBackward>)\n",
      "  7%|▋         | 7/100 [05:29<1:12:53, 47.03s/it]\n",
      " Episode 7 | step 100 | reward 2.485499999999999 | loss 0.22146436241303036\n",
      "Q values: tensor([0.4633, 0.4704, 0.4581], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 200 | reward 17.196200000000005 | loss 4.152675799985786\n",
      "Q values: tensor([0.5612, 0.5195, 0.5377], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 300 | reward 36.739250000000006 | loss 12.368123356751653\n",
      "Q values: tensor([0.7881, 0.6953, 0.7110], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 400 | reward 40.963350000000034 | loss 13.629074150944575\n",
      "Q values: tensor([0.7948, 0.6821, 0.7559], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 500 | reward 38.80112500000004 | loss 14.096938004479526\n",
      "Q values: tensor([0.9250, 0.8438, 0.9195], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 600 | reward 31.97200000000002 | loss 15.380853643713081\n",
      "Q values: tensor([0.9556, 0.9852, 1.0320], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 700 | reward 17.24940000000001 | loss 18.047434971523913\n",
      "Q values: tensor([0.6287, 0.7232, 0.8502], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 772 | reward 11.447575000000011 | loss 20.16770465760534\n",
      "Q values: tensor([1.0112, 1.0936, 1.1162], grad_fn=<SelectBackward>)\n",
      "  8%|▊         | 8/100 [06:13<1:10:46, 46.15s/it]\n",
      " Episode 8 | step 100 | reward -7.52605 | loss 1.2779439207070027\n",
      "Q values: tensor([0.5038, 0.5612, 0.4954], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 200 | reward -25.136325000000003 | loss 7.244422315106334\n",
      "Q values: tensor([0.4922, 0.5746, 0.5891], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 300 | reward -50.76797499999999 | loss 15.057714280020349\n",
      "Q values: tensor([0.4516, 0.5636, 0.6766], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 400 | reward -88.89372499999999 | loss 25.20472300522951\n",
      "Q values: tensor([0.2670, 0.4199, 0.6916], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 500 | reward -120.19745 | loss 32.62254414133125\n",
      "Q values: tensor([0.3784, 0.6505, 0.8304], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 600 | reward -149.77700000000002 | loss 37.727451310599264\n",
      "Q values: tensor([0.3592, 0.6609, 0.8805], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 700 | reward -178.0233250000001 | loss 41.97745489921681\n",
      "Q values: tensor([0.0897, 0.3478, 0.6865], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 793 | reward -209.19892500000006 | loss 49.19369419280571\n",
      "Q values: tensor([0.1580, 0.6438, 0.7338], grad_fn=<SelectBackward>)\n",
      "  9%|▉         | 9/100 [07:01<1:10:59, 46.81s/it]\n",
      " Episode 9 | step 100 | reward 0.9303249999999998 | loss 0.9605600039837867\n",
      "Q values: tensor([0.0932, 0.2594, 0.2580], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 200 | reward -0.5967750000000002 | loss 1.623748809444863\n",
      "Q values: tensor([0.0510, 0.2074, 0.2218], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 300 | reward -7.570675 | loss 1.9789704093898886\n",
      "Q values: tensor([0.0047, 0.1637, 0.2106], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 400 | reward -5.2459000000000024 | loss 3.460314428813775\n",
      "Q values: tensor([0.1604, 0.2446, 0.2486], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 500 | reward -2.864150000000002 | loss 4.005561925350762\n",
      "Q values: tensor([0.2204, 0.2518, 0.2519], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 600 | reward -11.279 | loss 4.92666493768845\n",
      "Q values: tensor([-0.0147,  0.0962,  0.1413], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 700 | reward -34.38997500000001 | loss 9.266298817246728\n",
      "Q values: tensor([-0.3304, -0.0923,  0.0679], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 779 | reward -57.90214999999998 | loss 10.965761011874207\n",
      "Q values: tensor([-0.3920, -0.2448,  0.1011], grad_fn=<SelectBackward>)\n",
      " 10%|█         | 10/100 [07:48<1:09:59, 46.66s/it]\n",
      " Episode 10 | step 100 | reward -6.529100000000001 | loss 1.6090038721345081\n",
      "Q values: tensor([-0.0936, -0.0762,  0.0091], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 200 | reward -32.66897500000001 | loss 8.236035581499705\n",
      "Q values: tensor([-0.2810, -0.1524,  0.0093], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 300 | reward -59.023000000000025 | loss 13.111658147621851\n",
      "Q values: tensor([-0.5462, -0.2937, -0.1204], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 400 | reward -91.31117500000005 | loss 16.707703046898416\n",
      "Q values: tensor([-0.7498, -0.4196, -0.1627], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 500 | reward -105.11705000000003 | loss 20.161535661902633\n",
      "Q values: tensor([-0.6443, -0.4119, -0.2087], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 600 | reward -116.54637500000008 | loss 21.748201106057223\n",
      "Q values: tensor([-0.6426, -0.4283, -0.2875], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 700 | reward -144.0367500000001 | loss 24.0765209721105\n",
      "Q values: tensor([-0.9509, -0.6944, -0.4782], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 800 | reward -163.7253000000001 | loss 25.364113883300206\n",
      "Q values: tensor([-0.9408, -0.8419, -0.5047], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 818 | reward -168.27935000000008 | loss 25.553893293249757\n",
      "Q values: tensor([-0.5950, -0.4547, -0.2803], grad_fn=<SelectBackward>)\n",
      " 11%|█         | 11/100 [08:37<1:10:29, 47.52s/it]\n",
      " Episode 11 | step 100 | reward 6.592875000000003 | loss 2.747008355714712\n",
      "Q values: tensor([-0.0189,  0.0259,  0.0742], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 200 | reward 3.937599999999999 | loss 3.0886220955752663\n",
      "Q values: tensor([-0.0230, -0.0033,  0.0783], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 300 | reward -7.600675000000001 | loss 4.034236626734829\n",
      "Q values: tensor([-0.1437, -0.0730,  0.0359], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 400 | reward -25.461675 | loss 6.137335516286955\n",
      "Q values: tensor([-0.3104, -0.1455, -0.0293], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 500 | reward -44.978699999999975 | loss 7.804199423485473\n",
      "Q values: tensor([-0.4300, -0.1904, -0.0924], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 600 | reward -64.28029999999998 | loss 10.064791800903523\n",
      "Q values: tensor([-0.6896, -0.3949, -0.2333], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 700 | reward -88.26394999999995 | loss 12.42762400818046\n",
      "Q values: tensor([-0.5877, -0.3335, -0.2657], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 800 | reward -101.25487499999997 | loss 13.519758705459795\n",
      "Q values: tensor([-0.7683, -0.6540, -0.5538], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 846 | reward -105.36902499999997 | loss 13.812305574249793\n",
      "Q values: tensor([-0.6430, -0.5201, -0.4628], grad_fn=<SelectBackward>)\n",
      " 12%|█▏        | 12/100 [09:29<1:11:38, 48.85s/it]\n",
      " Episode 12 | step 100 | reward -1.4022249999999998 | loss 0.14290681375561576\n",
      "Q values: tensor([0.0972, 0.1652, 0.1753], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 200 | reward -2.7422749999999994 | loss 0.337670704223207\n",
      "Q values: tensor([0.2269, 0.2681, 0.2944], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 300 | reward 0.9344000000000026 | loss 0.9006777178808321\n",
      "Q values: tensor([0.3171, 0.3040, 0.3371], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 400 | reward -2.955599999999996 | loss 2.084072446537995\n",
      "Q values: tensor([0.2678, 0.2786, 0.3575], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 500 | reward -21.563374999999994 | loss 5.185949787330777\n",
      "Q values: tensor([0.1675, 0.3095, 0.4395], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 600 | reward -43.711624999999984 | loss 7.916141558567013\n",
      "Q values: tensor([-0.1306,  0.1845,  0.3981], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 700 | reward -79.97654999999999 | loss 11.588143627338475\n",
      "Q values: tensor([-0.4598,  0.0743,  0.2808], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 778 | reward -109.55499999999995 | loss 17.02817971234547\n",
      "Q values: tensor([-0.4693,  0.0790,  0.1110], grad_fn=<SelectBackward>)\n",
      " 13%|█▎        | 13/100 [10:14<1:09:14, 47.75s/it]\n",
      " Episode 13 | step 100 | reward 4.962749999999999 | loss 1.43214162110017\n",
      "Q values: tensor([-0.1134, -0.0456, -0.0798], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 200 | reward 12.478000000000005 | loss 3.2195101687025405\n",
      "Q values: tensor([-0.1155, -0.1046, -0.1208], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 300 | reward 30.657400000000006 | loss 9.729475342428273\n",
      "Q values: tensor([-0.0171, -0.1238, -0.1493], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 400 | reward 42.645825 | loss 15.509677729329056\n",
      "Q values: tensor([ 0.0505, -0.1266, -0.0990], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 500 | reward 55.98907500000003 | loss 22.3793521213736\n",
      "Q values: tensor([ 0.1821, -0.0500, -0.0291], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 600 | reward 86.27722500000003 | loss 53.994516520520634\n",
      "Q values: tensor([1.0439, 0.5978, 0.6262], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 700 | reward 109.63512499999999 | loss 82.70971132511455\n",
      "Q values: tensor([1.4122, 0.9507, 1.0404], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 735 | reward 116.39027499999996 | loss 89.65485754491944\n",
      "Q values: tensor([1.8105, 1.3883, 1.4364], grad_fn=<SelectBackward>)\n",
      " 14%|█▍        | 14/100 [10:58<1:06:37, 46.48s/it]\n",
      " Episode 14 | step 100 | reward -3.5167500000000005 | loss 0.8078191002045969\n",
      "Q values: tensor([0.7506, 0.6541, 0.6832], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 200 | reward -11.141900000000001 | loss 2.365032853694629\n",
      "Q values: tensor([0.8134, 0.7555, 0.7959], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 300 | reward -24.120200000000008 | loss 5.843182414410421\n",
      "Q values: tensor([0.6762, 0.6931, 0.7279], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 400 | reward -33.75780000000001 | loss 7.39944930948997\n",
      "Q values: tensor([0.8189, 0.8644, 0.9110], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 500 | reward -42.93555000000001 | loss 8.336297288846428\n",
      "Q values: tensor([0.9412, 1.0096, 1.0684], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 600 | reward -62.426625000000016 | loss 16.745768346915103\n",
      "Q values: tensor([1.1464, 1.3008, 1.3749], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 700 | reward -84.85932500000007 | loss 22.764946483026907\n",
      "Q values: tensor([0.6285, 0.7916, 0.9778], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 799 | reward -107.46617500000005 | loss 28.715246795285285\n",
      "Q values: tensor([0.9679, 1.0471, 1.2747], grad_fn=<SelectBackward>)\n",
      " 15%|█▌        | 15/100 [11:38<1:03:04, 44.52s/it]\n",
      " Episode 15 | step 100 | reward 0.4373749999999998 | loss 0.45544550794738825\n",
      "Q values: tensor([0.4093, 0.4567, 0.5086], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 200 | reward 2.4141 | loss 1.0694900793951234\n",
      "Q values: tensor([0.6315, 0.6047, 0.6816], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 300 | reward 2.8908 | loss 1.5972312157237099\n",
      "Q values: tensor([0.6994, 0.6634, 0.7303], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 400 | reward 13.85685 | loss 5.758666180463493\n",
      "Q values: tensor([1.2225, 1.0731, 1.0872], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 500 | reward 24.070550000000004 | loss 9.360184187402652\n",
      "Q values: tensor([1.0159, 0.8922, 0.9343], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 600 | reward 27.357224999999996 | loss 10.524504195254863\n",
      "Q values: tensor([1.0664, 0.9683, 1.0359], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 700 | reward 36.424475 | loss 16.15058793671507\n",
      "Q values: tensor([2.1102, 2.0253, 1.9130], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 733 | reward 42.274100000000004 | loss 22.116146710644003\n",
      "Q values: tensor([2.2718, 2.0911, 2.0445], grad_fn=<SelectBackward>)\n",
      " 16%|█▌        | 16/100 [12:20<1:01:33, 43.97s/it]\n",
      " Episode 16 | step 100 | reward 0.3975249999999999 | loss 0.5220895979624629\n",
      "Q values: tensor([0.6152, 0.6810, 0.6212], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 200 | reward 1.737275 | loss 0.900794061599413\n",
      "Q values: tensor([0.6997, 0.7567, 0.7196], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 300 | reward -2.4475999999999996 | loss 1.3132622354710293\n",
      "Q values: tensor([0.7271, 0.7906, 0.7839], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 400 | reward -5.200349999999999 | loss 1.6456123068088004\n",
      "Q values: tensor([0.7993, 0.8668, 0.8702], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 500 | reward -3.7979999999999983 | loss 2.349890626950412\n",
      "Q values: tensor([1.1776, 1.2049, 1.2037], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 600 | reward -7.570099999999997 | loss 4.431503831364694\n",
      "Q values: tensor([1.1424, 1.2581, 1.2311], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 700 | reward -20.99302499999999 | loss 6.492058936723083\n",
      "Q values: tensor([1.2133, 1.3478, 1.4479], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 749 | reward -25.267074999999988 | loss 9.304888094198873\n",
      "Q values: tensor([0.9897, 1.1135, 1.2460], grad_fn=<SelectBackward>)\n",
      " 17%|█▋        | 17/100 [13:05<1:00:55, 44.04s/it]\n",
      " Episode 17 | step 100 | reward -2.39505 | loss 0.37486559788915486\n",
      "Q values: tensor([0.5530, 0.5297, 0.5453], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 200 | reward -9.797274999999996 | loss 2.0153692908461522\n",
      "Q values: tensor([0.4646, 0.4541, 0.5120], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 300 | reward -12.136249999999997 | loss 3.432693447077341\n",
      "Q values: tensor([0.6192, 0.6158, 0.6597], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 400 | reward -8.79314999999999 | loss 4.224025551014918\n",
      "Q values: tensor([1.1289, 1.0811, 1.0705], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 500 | reward -18.81534999999999 | loss 6.686410387580921\n",
      "Q values: tensor([0.5832, 0.5956, 0.6867], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 600 | reward -24.532324999999993 | loss 7.441661535796541\n",
      "Q values: tensor([1.1374, 1.1635, 1.1929], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 700 | reward -31.608424999999993 | loss 9.44373503845084\n",
      "Q values: tensor([1.2289, 1.2922, 1.2773], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 800 | reward -46.0211 | loss 11.743398276914675\n",
      "Q values: tensor([0.4185, 0.5212, 0.6375], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 814 | reward -46.3157 | loss 12.07068023023983\n",
      "Q values: tensor([0.2532, 0.2870, 0.3940], grad_fn=<SelectBackward>)\n",
      " 18%|█▊        | 18/100 [13:52<1:01:33, 45.04s/it]\n",
      " Episode 18 | step 100 | reward 0.7168749999999995 | loss 0.3353727399118185\n",
      "Q values: tensor([0.5400, 0.5488, 0.5275], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 200 | reward 0.009074999999999514 | loss 0.53193869978519\n",
      "Q values: tensor([0.6221, 0.6265, 0.6097], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 300 | reward -5.709625000000002 | loss 1.5668213272478972\n",
      "Q values: tensor([0.4743, 0.5153, 0.5308], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 400 | reward -14.721475000000002 | loss 3.1095317658259667\n",
      "Q values: tensor([0.3698, 0.4482, 0.5138], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 500 | reward -29.991400000000002 | loss 4.712748182564827\n",
      "Q values: tensor([0.3093, 0.4186, 0.5193], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 600 | reward -41.608149999999995 | loss 6.43639919867427\n",
      "Q values: tensor([0.8668, 0.9476, 1.0443], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 700 | reward -59.152249999999995 | loss 8.17499643720156\n",
      "Q values: tensor([0.5252, 0.6091, 0.8249], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 770 | reward -57.70902500000001 | loss 11.678340991484664\n",
      "Q values: tensor([1.0220, 0.9283, 0.9574], grad_fn=<SelectBackward>)\n",
      " 19%|█▉        | 19/100 [14:37<1:00:46, 45.01s/it]\n",
      " Episode 19 | step 100 | reward 7.797375000000001 | loss 2.0208677740106396\n",
      "Q values: tensor([0.3122, 0.3612, 0.3339], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 200 | reward 21.149250000000002 | loss 5.745366407140566\n",
      "Q values: tensor([0.4356, 0.4355, 0.3956], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 300 | reward 41.60730000000003 | loss 13.120853188561043\n",
      "Q values: tensor([0.8126, 0.7940, 0.6861], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 400 | reward 61.55672500000003 | loss 22.548393235928117\n",
      "Q values: tensor([1.0408, 0.9264, 0.8459], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 500 | reward 68.72440000000003 | loss 24.876027644835162\n",
      "Q values: tensor([0.7743, 0.6761, 0.6389], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 600 | reward 97.54615 | loss 44.769550502407114\n",
      "Q values: tensor([1.9834, 1.7941, 1.5747], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 700 | reward 129.34107500000002 | loss 71.81603530466008\n",
      "Q values: tensor([2.8152, 2.5827, 2.4707], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 728 | reward 138.54987499999996 | loss 86.44009615915046\n",
      "Q values: tensor([3.2196, 2.9587, 2.8411], grad_fn=<SelectBackward>)\n",
      " 20%|██        | 20/100 [15:19<59:02, 44.28s/it]  \n",
      " Episode 20 | step 100 | reward -2.139474999999999 | loss 0.39800762645000987\n",
      "Q values: tensor([0.9528, 0.9328, 0.8739], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 200 | reward -3.08135 | loss 0.6869014493180998\n",
      "Q values: tensor([1.2479, 1.2043, 1.1889], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 300 | reward -10.349000000000004 | loss 2.584809560415522\n",
      "Q values: tensor([1.3140, 1.2886, 1.2888], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 400 | reward -24.17375000000001 | loss 7.8235464412841225\n",
      "Q values: tensor([1.5419, 1.5319, 1.5646], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 500 | reward -44.95612500000002 | loss 16.631074900254532\n",
      "Q values: tensor([1.4306, 1.4448, 1.4954], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 600 | reward -50.71337500000003 | loss 18.46250918275649\n",
      "Q values: tensor([1.8236, 1.8365, 1.8914], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 700 | reward -57.69932500000004 | loss 21.356461045508638\n",
      "Q values: tensor([2.0834, 2.1135, 2.1162], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 779 | reward -65.41592500000006 | loss 29.1725763224202\n",
      "Q values: tensor([1.9560, 2.0262, 2.0499], grad_fn=<SelectBackward>)\n",
      " 21%|██        | 21/100 [16:10<1:00:46, 46.15s/it]\n",
      " Episode 21 | step 100 | reward 5.970099999999998 | loss 1.488654227102458\n",
      "Q values: tensor([0.7851, 0.8436, 0.8535], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 200 | reward 16.049575 | loss 3.9251755998184734\n",
      "Q values: tensor([0.9678, 1.0067, 0.9966], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 300 | reward 36.119024999999986 | loss 10.360137238222066\n",
      "Q values: tensor([1.3930, 1.4011, 1.3290], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 400 | reward 57.23972499999998 | loss 18.19248378860975\n",
      "Q values: tensor([2.0741, 2.0266, 1.8954], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 500 | reward 66.62404999999998 | loss 21.31120336864079\n",
      "Q values: tensor([2.0125, 1.9411, 1.9006], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 600 | reward 69.70754999999993 | loss 24.240419981680972\n",
      "Q values: tensor([2.2448, 2.2068, 2.2100], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 700 | reward 69.93552499999993 | loss 25.70684792944796\n",
      "Q values: tensor([2.4739, 2.4597, 2.4242], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 739 | reward 70.17467499999994 | loss 34.019678050569766\n",
      "Q values: tensor([2.5947, 2.5688, 2.5609], grad_fn=<SelectBackward>)\n",
      " 22%|██▏       | 22/100 [16:48<56:54, 43.78s/it]  \n",
      " Episode 22 | step 100 | reward 0.03505000000000007 | loss 0.32596659711816756\n",
      "Q values: tensor([0.9532, 0.9218, 0.9014], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 200 | reward 6.61665 | loss 1.1247837756573063\n",
      "Q values: tensor([1.1851, 1.1128, 1.0744], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 300 | reward 9.047549999999996 | loss 2.2188081612364385\n",
      "Q values: tensor([1.4419, 1.4125, 1.4188], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 400 | reward 5.220074999999993 | loss 3.33626310619303\n",
      "Q values: tensor([1.5432, 1.5463, 1.5574], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 500 | reward -3.8034750000000077 | loss 8.353682138401666\n",
      "Q values: tensor([1.6754, 1.7731, 1.7633], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 600 | reward -22.851325000000006 | loss 14.201988213099469\n",
      "Q values: tensor([1.5125, 1.6133, 1.7347], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 700 | reward -32.489774999999995 | loss 16.254502265398116\n",
      "Q values: tensor([2.3445, 2.4918, 2.5256], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 800 | reward -45.994175000000034 | loss 21.215056220415647\n",
      "Q values: tensor([2.2476, 2.4510, 2.4692], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 831 | reward -50.410575000000044 | loss 29.896032659755804\n",
      "Q values: tensor([1.6813, 1.8510, 1.8976], grad_fn=<SelectBackward>)\n",
      " 23%|██▎       | 23/100 [17:34<57:07, 44.52s/it]\n",
      " Episode 23 | step 100 | reward 4.223450000000001 | loss 0.5186839550337226\n",
      "Q values: tensor([0.7691, 0.7486, 0.7355], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 200 | reward 14.321549999999997 | loss 2.5015165134375543\n",
      "Q values: tensor([0.9765, 0.9373, 0.9178], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 300 | reward 17.96174999999999 | loss 2.9979966592275282\n",
      "Q values: tensor([0.9538, 0.9351, 0.9262], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 400 | reward 39.327824999999976 | loss 11.057551187752797\n",
      "Q values: tensor([1.5118, 1.4138, 1.3771], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 500 | reward 63.21977499999998 | loss 23.27745651532179\n",
      "Q values: tensor([1.7312, 1.6134, 1.5696], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 600 | reward 79.62722500000002 | loss 31.53666857821965\n",
      "Q values: tensor([2.1637, 2.0100, 2.0207], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 700 | reward 82.52537499999998 | loss 32.43241577438518\n",
      "Q values: tensor([2.0496, 1.9758, 2.0036], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 785 | reward 93.09799999999998 | loss 43.92249075483711\n",
      "Q values: tensor([3.0440, 2.9544, 2.8656], grad_fn=<SelectBackward>)\n",
      " 24%|██▍       | 24/100 [18:23<58:00, 45.79s/it]\n",
      " Episode 24 | step 100 | reward 4.553899999999997 | loss 1.4447699510603798\n",
      "Q values: tensor([1.5528, 1.4260, 1.4336], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 200 | reward 8.047024999999996 | loss 2.1877674296204077\n",
      "Q values: tensor([1.7075, 1.5964, 1.6063], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 300 | reward 11.5322 | loss 3.1805512534700853\n",
      "Q values: tensor([1.9342, 1.8418, 1.8523], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 400 | reward 15.540299999999998 | loss 4.512049548157634\n",
      "Q values: tensor([2.2966, 2.2176, 2.1999], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 500 | reward 30.92664999999998 | loss 9.3991461420717\n",
      "Q values: tensor([3.1060, 2.9780, 2.8737], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 600 | reward 50.20452499999999 | loss 16.770159758821833\n",
      "Q values: tensor([3.1668, 3.0631, 2.9497], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 700 | reward 72.92737500000001 | loss 28.159148571690608\n",
      "Q values: tensor([4.7074, 4.4426, 4.4261], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 751 | reward 77.54597500000001 | loss 53.06481857325595\n",
      "Q values: tensor([4.7528, 4.6132, 4.6073], grad_fn=<SelectBackward>)\n",
      " 25%|██▌       | 25/100 [19:08<56:57, 45.56s/it]\n",
      " Episode 25 | step 100 | reward 0.015875000000000264 | loss 0.4453267572698678\n",
      "Q values: tensor([1.2500, 1.2841, 1.2529], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 200 | reward 1.026875 | loss 0.7761776866227592\n",
      "Q values: tensor([1.4228, 1.4575, 1.4484], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 300 | reward -15.288224999999997 | loss 7.475147777610232\n",
      "Q values: tensor([1.3108, 1.3679, 1.4080], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 400 | reward -31.847999999999985 | loss 11.002811850472767\n",
      "Q values: tensor([1.4199, 1.5167, 1.5910], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 500 | reward -36.352949999999986 | loss 12.367927431189088\n",
      "Q values: tensor([2.0660, 2.1490, 2.1815], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 600 | reward -47.42337499999997 | loss 16.25884234758601\n",
      "Q values: tensor([2.1766, 2.2744, 2.2990], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 700 | reward -68.72539999999992 | loss 22.538086192764837\n",
      "Q values: tensor([1.8638, 2.0037, 2.1377], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 800 | reward -83.44007499999991 | loss 26.13082178067353\n",
      "Q values: tensor([2.8979, 2.9812, 3.0426], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 804 | reward -83.8963249999999 | loss 37.22474222166866\n",
      "Q values: tensor([2.8690, 2.9429, 2.9869], grad_fn=<SelectBackward>)\n",
      " 26%|██▌       | 26/100 [20:03<59:37, 48.34s/it]\n",
      " Episode 26 | step 100 | reward 0.35307499999999997 | loss 0.6296532553606085\n",
      "Q values: tensor([1.2429, 1.3209, 1.2568], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 200 | reward -2.1606750000000003 | loss 1.4661494012079856\n",
      "Q values: tensor([1.1594, 1.2155, 1.1903], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 300 | reward -13.463450000000002 | loss 4.465161839808559\n",
      "Q values: tensor([1.2708, 1.3279, 1.3274], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 400 | reward -29.871524999999995 | loss 8.557651088645514\n",
      "Q values: tensor([1.3043, 1.3476, 1.4251], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 500 | reward -35.36197499999996 | loss 9.868250897390396\n",
      "Q values: tensor([1.8014, 1.8358, 1.8802], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 600 | reward -35.56639999999995 | loss 11.211152083995955\n",
      "Q values: tensor([1.7165, 1.7389, 1.7649], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 700 | reward -28.864874999999962 | loss 12.977188853499854\n",
      "Q values: tensor([2.4688, 2.4613, 2.3993], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 752 | reward -28.502874999999968 | loss 22.248136985984445\n",
      "Q values: tensor([2.7829, 2.7531, 2.7119], grad_fn=<SelectBackward>)\n",
      " 27%|██▋       | 27/100 [20:51<58:37, 48.18s/it]\n",
      " Episode 27 | step 100 | reward 3.0924499999999995 | loss 0.5126556205350568\n",
      "Q values: tensor([0.9588, 0.9350, 0.9257], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 200 | reward -3.14715 | loss 2.846241509938878\n",
      "Q values: tensor([1.1336, 1.1252, 1.1314], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 300 | reward -28.018650000000004 | loss 14.647807207131862\n",
      "Q values: tensor([0.8737, 0.9515, 1.0077], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 400 | reward -60.66407499999999 | loss 25.457336894351627\n",
      "Q values: tensor([0.7962, 0.9207, 1.0599], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 500 | reward -82.28970000000004 | loss 29.57732435343958\n",
      "Q values: tensor([1.1509, 1.3113, 1.4710], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 600 | reward -111.45607500000004 | loss 36.930415465093645\n",
      "Q values: tensor([0.7727, 1.0290, 1.2411], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 700 | reward -150.00165000000004 | loss 50.27108126400442\n",
      "Q values: tensor([1.3137, 1.6676, 1.8211], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 785 | reward -183.53897500000016 | loss 62.35473002664101\n",
      "Q values: tensor([0.9413, 1.4383, 1.6445], grad_fn=<SelectBackward>)\n",
      " 28%|██▊       | 28/100 [21:41<58:40, 48.89s/it]\n",
      " Episode 28 | step 100 | reward 4.091149999999999 | loss 4.203175117701903\n",
      "Q values: tensor([0.4762, 0.6710, 0.7236], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 200 | reward -0.17427500000000037 | loss 5.882289596422197\n",
      "Q values: tensor([0.6945, 0.8781, 0.9096], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 300 | reward -6.380650000000001 | loss 6.945753074875711\n",
      "Q values: tensor([0.6427, 0.8194, 0.8628], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 400 | reward -15.916225000000011 | loss 7.923312539014432\n",
      "Q values: tensor([0.6323, 0.8018, 0.8540], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 500 | reward -19.07595000000001 | loss 9.250946803689203\n",
      "Q values: tensor([0.8051, 0.9595, 0.9440], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 600 | reward -31.450750000000006 | loss 13.14633638895532\n",
      "Q values: tensor([0.3652, 0.5741, 0.6407], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 700 | reward -57.539500000000025 | loss 20.524979945808596\n",
      "Q values: tensor([0.2060, 0.5750, 0.6825], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 791 | reward -95.91405000000003 | loss 30.40244933399474\n",
      "Q values: tensor([0.2343, 0.6029, 0.8716], grad_fn=<SelectBackward>)\n",
      " 29%|██▉       | 29/100 [22:32<58:27, 49.40s/it]\n",
      " Episode 29 | step 100 | reward 3.1236499999999996 | loss 1.6479030664674426\n",
      "Q values: tensor([0.3880, 0.4581, 0.4900], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 200 | reward 12.739225000000001 | loss 5.040568726493685\n",
      "Q values: tensor([0.6177, 0.7190, 0.7097], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 300 | reward 17.85932500000001 | loss 6.886739784822449\n",
      "Q values: tensor([0.2578, 0.3476, 0.4077], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 400 | reward 33.23017500000001 | loss 14.397115779004533\n",
      "Q values: tensor([0.8103, 0.8774, 0.8087], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 500 | reward 51.80090000000001 | loss 21.832714288382984\n",
      "Q values: tensor([0.6696, 0.6980, 0.6813], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 600 | reward 74.12060000000008 | loss 30.27618865320761\n",
      "Q values: tensor([0.9864, 0.9132, 0.8547], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 700 | reward 100.35535000000004 | loss 48.1876484537766\n",
      "Q values: tensor([1.9427, 1.7424, 1.7523], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 769 | reward 119.96070000000006 | loss 67.47319137272785\n",
      "Q values: tensor([3.1071, 2.8148, 2.7652], grad_fn=<SelectBackward>)\n",
      " 30%|███       | 30/100 [23:22<57:43, 49.48s/it]\n",
      " Episode 30 | step 100 | reward 5.5108500000000005 | loss 1.6213301511163536\n",
      "Q values: tensor([1.0076, 0.9130, 0.8651], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 200 | reward 15.420249999999998 | loss 4.497677717592978\n",
      "Q values: tensor([1.4262, 1.3444, 1.2944], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 300 | reward 28.736299999999993 | loss 8.76045328479313\n",
      "Q values: tensor([1.8824, 1.8170, 1.7447], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 400 | reward 31.991475000000015 | loss 10.454314111342114\n",
      "Q values: tensor([1.6357, 1.5773, 1.5819], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 500 | reward 27.90510000000001 | loss 12.51333620424056\n",
      "Q values: tensor([1.8990, 1.8730, 1.8662], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 600 | reward 21.767950000000006 | loss 14.528091682703135\n",
      "Q values: tensor([1.8228, 1.8054, 1.8429], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 700 | reward 32.40640000000001 | loss 19.07678064251276\n",
      "Q values: tensor([3.1990, 3.1517, 3.0392], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 745 | reward 40.27200000000001 | loss 27.04222484478477\n",
      "Q values: tensor([2.7018, 2.6539, 2.6281], grad_fn=<SelectBackward>)\n",
      " 31%|███       | 31/100 [24:07<55:30, 48.27s/it]\n",
      " Episode 31 | step 100 | reward 1.186750000000001 | loss 0.8892736068883664\n",
      "Q values: tensor([1.5126, 1.5132, 1.4546], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 200 | reward -3.055675 | loss 2.578902302706979\n",
      "Q values: tensor([1.5306, 1.5411, 1.5124], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 300 | reward -9.096475000000003 | loss 4.536721594031988\n",
      "Q values: tensor([1.6878, 1.6926, 1.6895], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 400 | reward -9.118800000000002 | loss 5.526418588852962\n",
      "Q values: tensor([2.1311, 2.1262, 2.1095], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 500 | reward -16.7704 | loss 8.219446533491706\n",
      "Q values: tensor([1.8568, 1.8974, 1.8954], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 600 | reward -25.36179999999999 | loss 11.008910609235741\n",
      "Q values: tensor([2.3418, 2.3258, 2.3084], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 700 | reward -27.09194999999998 | loss 12.893259442957529\n",
      "Q values: tensor([2.6448, 2.6418, 2.6205], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 762 | reward -32.85749999999999 | loss 21.812427158568894\n",
      "Q values: tensor([2.1340, 2.2096, 2.2566], grad_fn=<SelectBackward>)\n",
      " 32%|███▏      | 32/100 [24:52<53:39, 47.34s/it]\n",
      " Episode 32 | step 100 | reward -5.575574999999999 | loss 0.9515254888277695\n",
      "Q values: tensor([0.7176, 0.6530, 0.6709], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 200 | reward -3.288774999999999 | loss 1.5094303771562636\n",
      "Q values: tensor([0.8591, 0.8259, 0.8334], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 300 | reward 3.288275000000001 | loss 2.6737941042466673\n",
      "Q values: tensor([1.1226, 1.0792, 1.0702], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 400 | reward 13.658975000000002 | loss 4.943686118575954\n",
      "Q values: tensor([1.2828, 1.1810, 1.2056], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 500 | reward 11.558900000000003 | loss 6.084359686427035\n",
      "Q values: tensor([1.1424, 1.0614, 1.1700], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 600 | reward 4.904349999999999 | loss 7.0761006899275\n",
      "Q values: tensor([1.1099, 1.1017, 1.2207], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 700 | reward 9.699024999999995 | loss 8.193596693743473\n",
      "Q values: tensor([1.8784, 1.8627, 1.8355], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 777 | reward 10.986849999999999 | loss 14.868724107861851\n",
      "Q values: tensor([2.1525, 2.1438, 2.0550], grad_fn=<SelectBackward>)\n",
      " 33%|███▎      | 33/100 [25:39<52:34, 47.08s/it]\n",
      " Episode 33 | step 100 | reward 4.694800000000001 | loss 0.7003288384796917\n",
      "Q values: tensor([0.8946, 0.9352, 0.8950], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 200 | reward 12.617150000000006 | loss 2.101773031943054\n",
      "Q values: tensor([1.1474, 1.1798, 1.1398], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 300 | reward 18.687025000000002 | loss 3.666477364026889\n",
      "Q values: tensor([1.2912, 1.3189, 1.2697], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 400 | reward 14.784325 | loss 4.468458216984175\n",
      "Q values: tensor([1.0648, 1.0959, 1.0857], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 500 | reward 18.542424999999987 | loss 4.875400201142213\n",
      "Q values: tensor([1.5952, 1.6121, 1.5626], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 600 | reward 18.52222499999998 | loss 6.206836217153352\n",
      "Q values: tensor([1.5953, 1.6220, 1.5989], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 700 | reward 14.978899999999982 | loss 7.513287559971843\n",
      "Q values: tensor([1.5140, 1.5060, 1.5224], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 774 | reward 18.62549999999998 | loss 11.954925987085346\n",
      "Q values: tensor([1.9545, 1.9100, 1.9288], grad_fn=<SelectBackward>)\n",
      " 34%|███▍      | 34/100 [26:24<51:08, 46.49s/it]\n",
      " Episode 34 | step 100 | reward -2.5843499999999993 | loss 0.6383950907262488\n",
      "Q values: tensor([1.3046, 1.3521, 1.3699], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 200 | reward -12.441574999999998 | loss 2.9214761862521073\n",
      "Q values: tensor([1.0258, 1.1374, 1.1454], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 300 | reward -28.984949999999998 | loss 6.253054781403421\n",
      "Q values: tensor([0.9388, 1.0039, 1.0903], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 400 | reward -29.225374999999993 | loss 7.077455902210545\n",
      "Q values: tensor([1.4146, 1.4557, 1.4584], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 500 | reward -32.96357499999998 | loss 8.190917109251473\n",
      "Q values: tensor([1.4361, 1.4670, 1.4686], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 600 | reward -47.87252499999998 | loss 12.041836684443197\n",
      "Q values: tensor([1.1726, 1.2721, 1.3438], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 700 | reward -63.067349999999976 | loss 14.214539681523448\n",
      "Q values: tensor([1.0265, 1.1016, 1.2194], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 792 | reward -65.25309999999996 | loss 19.268689585519624\n",
      "Q values: tensor([1.6895, 1.7674, 1.8027], grad_fn=<SelectBackward>)\n",
      " 35%|███▌      | 35/100 [27:13<51:15, 47.32s/it]\n",
      " Episode 35 | step 100 | reward -2.983425 | loss 0.14263348829780753\n",
      "Q values: tensor([0.6190, 0.6548, 0.6807], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 200 | reward 0.9057750000000001 | loss 0.7482392755347895\n",
      "Q values: tensor([0.8377, 0.8883, 0.9021], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 300 | reward 10.105425000000004 | loss 2.400352967687784\n",
      "Q values: tensor([0.9565, 1.0092, 0.9730], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 400 | reward 12.763025000000003 | loss 3.314350501475335\n",
      "Q values: tensor([0.8987, 0.9313, 0.9105], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 500 | reward 9.512425000000002 | loss 3.918575322409513\n",
      "Q values: tensor([1.0091, 1.0103, 1.0135], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 600 | reward 3.1228250000000015 | loss 5.322673835671001\n",
      "Q values: tensor([0.8785, 0.9371, 0.9653], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 700 | reward -11.8446 | loss 7.804625047949546\n",
      "Q values: tensor([0.9896, 1.0919, 1.1715], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 749 | reward -18.9061 | loss 10.00182326541509\n",
      "Q values: tensor([0.8065, 0.8516, 0.9467], grad_fn=<SelectBackward>)\n",
      " 36%|███▌      | 36/100 [28:02<50:50, 47.66s/it]\n",
      " Episode 36 | step 100 | reward 2.9283750000000004 | loss 0.24696794823699975\n",
      "Q values: tensor([0.5693, 0.5318, 0.5166], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 200 | reward 12.445774999999996 | loss 1.7808929986164266\n",
      "Q values: tensor([0.6873, 0.6673, 0.6546], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 300 | reward 21.866449999999997 | loss 3.7612979392471324\n",
      "Q values: tensor([0.8864, 0.8583, 0.8406], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 400 | reward 25.45145 | loss 4.127240907205621\n",
      "Q values: tensor([0.7652, 0.7275, 0.7314], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 500 | reward 25.196075 | loss 4.331190407639188\n",
      "Q values: tensor([0.8688, 0.8456, 0.8570], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 600 | reward 22.47962500000001 | loss 4.838166140600354\n",
      "Q values: tensor([0.8444, 0.8428, 0.8854], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 700 | reward 17.486525000000007 | loss 5.596533231579759\n",
      "Q values: tensor([1.0944, 1.0990, 1.0724], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 800 | reward 12.22662500000002 | loss 6.483552642325538\n",
      "Q values: tensor([0.5292, 0.5574, 0.5970], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 812 | reward 12.741525000000022 | loss 6.992316702916231\n",
      "Q values: tensor([0.6428, 0.6404, 0.6451], grad_fn=<SelectBackward>)\n",
      " 37%|███▋      | 37/100 [28:52<50:55, 48.50s/it]\n",
      " Episode 37 | step 100 | reward 0.37275000000000014 | loss 0.17566517825709127\n",
      "Q values: tensor([0.5828, 0.6093, 0.5637], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 200 | reward -6.046400000000001 | loss 1.4294254009779905\n",
      "Q values: tensor([0.6076, 0.6588, 0.6694], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 300 | reward -16.495949999999997 | loss 2.6779278450161037\n",
      "Q values: tensor([0.5107, 0.5707, 0.6132], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 400 | reward -18.85677499999999 | loss 2.956276640971925\n",
      "Q values: tensor([0.5510, 0.5972, 0.6259], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 500 | reward -13.92724999999999 | loss 3.6796659936422067\n",
      "Q values: tensor([0.5546, 0.5918, 0.5438], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 600 | reward -2.5805499999999864 | loss 6.524823458002206\n",
      "Q values: tensor([1.1505, 1.0807, 0.9623], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 700 | reward 0.5477750000000149 | loss 7.770335325106348\n",
      "Q values: tensor([1.5158, 1.4826, 1.5028], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 765 | reward -2.711949999999985 | loss 10.052469081151319\n",
      "Q values: tensor([1.0811, 1.0687, 1.1415], grad_fn=<SelectBackward>)\n",
      " 38%|███▊      | 38/100 [29:37<49:08, 47.56s/it]\n",
      " Episode 38 | step 100 | reward 3.947975 | loss 0.28529591009248634\n",
      "Q values: tensor([0.7459, 0.7472, 0.7342], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 200 | reward 11.583774999999997 | loss 1.375152588528067\n",
      "Q values: tensor([0.8378, 0.8150, 0.7707], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 300 | reward 23.849125 | loss 4.356266872133006\n",
      "Q values: tensor([1.0627, 0.9767, 0.9510], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 400 | reward 28.93899999999997 | loss 5.0886363238274726\n",
      "Q values: tensor([0.9263, 0.8876, 0.8544], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 500 | reward 29.02997499999997 | loss 5.499935102739258\n",
      "Q values: tensor([1.0372, 1.0171, 1.0222], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 600 | reward 26.289899999999978 | loss 6.053054220960988\n",
      "Q values: tensor([1.0129, 1.0438, 1.0265], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 700 | reward 33.8027 | loss 7.839431993373367\n",
      "Q values: tensor([1.6274, 1.5378, 1.4754], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 746 | reward 39.2687 | loss 13.411784510330747\n",
      "Q values: tensor([2.0694, 2.0333, 1.9258], grad_fn=<SelectBackward>)\n",
      " 39%|███▉      | 39/100 [30:22<47:30, 46.74s/it]\n",
      " Episode 39 | step 100 | reward 3.5789500000000003 | loss 0.4901641471530702\n",
      "Q values: tensor([0.8946, 0.8841, 0.8318], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 200 | reward 10.620750000000003 | loss 1.360705358327511\n",
      "Q values: tensor([1.0658, 1.0510, 1.0108], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 300 | reward 14.145450000000004 | loss 1.7823346652195262\n",
      "Q values: tensor([1.1216, 1.1176, 1.0905], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 400 | reward 12.696700000000005 | loss 2.1390999144338174\n",
      "Q values: tensor([1.0059, 0.9922, 1.0249], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 500 | reward 8.756750000000016 | loss 2.6905487627535685\n",
      "Q values: tensor([1.2225, 1.2176, 1.2145], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 600 | reward 1.7103250000000165 | loss 3.700307875188063\n",
      "Q values: tensor([0.8555, 0.8646, 0.9665], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 700 | reward 15.864600000000019 | loss 8.929496358466512\n",
      "Q values: tensor([2.0438, 2.0088, 1.8997], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 751 | reward 27.202100000000023 | loss 16.95124005799966\n",
      "Q values: tensor([2.2560, 2.2973, 2.0855], grad_fn=<SelectBackward>)\n",
      " 40%|████      | 40/100 [31:13<47:57, 47.96s/it]\n",
      " Episode 40 | step 100 | reward -2.0195499999999997 | loss 0.370828910376094\n",
      "Q values: tensor([0.9510, 0.9463, 0.9065], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 200 | reward -0.9976499999999995 | loss 0.6135429006012778\n",
      "Q values: tensor([1.0645, 1.0696, 1.0443], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 300 | reward -4.898375 | loss 1.8915802103510124\n",
      "Q values: tensor([1.0607, 1.0589, 1.0632], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 400 | reward -19.885424999999994 | loss 6.1100745748847665\n",
      "Q values: tensor([0.9278, 0.9178, 0.9985], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 500 | reward -29.62985 | loss 8.071352577247826\n",
      "Q values: tensor([1.0552, 1.0558, 1.0960], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 600 | reward -30.918500000000012 | loss 9.007517854808643\n",
      "Q values: tensor([1.1357, 1.1730, 1.1937], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 700 | reward -30.549275000000012 | loss 10.311698234499122\n",
      "Q values: tensor([1.4060, 1.4250, 1.3731], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 800 | reward -50.53395000000003 | loss 17.025196109404565\n",
      "Q values: tensor([1.3735, 1.4535, 1.5135], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 805 | reward -52.12565000000003 | loss 22.112824105629475\n",
      "Q values: tensor([1.5776, 1.6740, 1.7136], grad_fn=<SelectBackward>)\n",
      " 41%|████      | 41/100 [32:07<48:51, 49.68s/it]\n",
      " Episode 41 | step 100 | reward 5.0042 | loss 1.3614803223273668\n",
      "Q values: tensor([0.7540, 0.7584, 0.8023], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 200 | reward 18.63405 | loss 5.300554831824428\n",
      "Q values: tensor([0.9540, 0.9538, 0.9329], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 300 | reward 43.575175 | loss 14.987786867545978\n",
      "Q values: tensor([1.0225, 1.0245, 0.9510], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 400 | reward 65.312825 | loss 23.807848742399287\n",
      "Q values: tensor([1.5398, 1.5561, 1.3912], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 500 | reward 83.53585 | loss 29.48399373324513\n",
      "Q values: tensor([1.2383, 1.2591, 1.2440], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 600 | reward 109.85087500000002 | loss 44.37849761718711\n",
      "Q values: tensor([2.2679, 2.3224, 2.1199], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 700 | reward 151.52595000000002 | loss 72.87570871673664\n",
      "Q values: tensor([2.6906, 2.6555, 2.5336], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 759 | reward 178.45075000000003 | loss 96.87652689341915\n",
      "Q values: tensor([3.0934, 2.9550, 2.8413], grad_fn=<SelectBackward>)\n",
      " 42%|████▏     | 42/100 [32:54<47:25, 49.07s/it]\n",
      " Episode 42 | step 100 | reward 1.5495499999999998 | loss 1.0027926611410294\n",
      "Q values: tensor([1.1454, 1.0874, 1.0997], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 200 | reward 3.758800000000002 | loss 2.0981911478289845\n",
      "Q values: tensor([1.3091, 1.2495, 1.2620], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 300 | reward -6.213124999999997 | loss 4.872413550234029\n",
      "Q values: tensor([1.1781, 1.1839, 1.2949], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 400 | reward -18.723175 | loss 9.475124431882165\n",
      "Q values: tensor([1.7353, 1.7181, 1.7124], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 500 | reward -36.857175000000005 | loss 14.914393781335235\n",
      "Q values: tensor([1.2292, 1.3765, 1.4890], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 600 | reward -68.71565000000005 | loss 32.399205945115824\n",
      "Q values: tensor([0.8404, 1.1533, 1.3708], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 700 | reward -111.00462500000006 | loss 43.27774804796813\n",
      "Q values: tensor([0.8250, 1.0528, 1.4067], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 784 | reward -139.75992499999995 | loss 51.35446683831883\n",
      "Q values: tensor([1.4025, 1.5209, 1.8431], grad_fn=<SelectBackward>)\n",
      " 43%|████▎     | 43/100 [33:36<44:21, 46.69s/it]\n",
      " Episode 43 | step 100 | reward -4.352050000000001 | loss 0.6975332851502003\n",
      "Q values: tensor([0.8136, 0.8310, 0.9307], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 200 | reward -5.016300000000003 | loss 1.1913959544074828\n",
      "Q values: tensor([0.9094, 0.9068, 0.9905], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 300 | reward -3.384600000000003 | loss 1.7461548346810503\n",
      "Q values: tensor([1.1058, 1.0882, 1.1553], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 400 | reward -5.115325000000002 | loss 2.4476088564159397\n",
      "Q values: tensor([0.9179, 0.9507, 1.0342], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 500 | reward 9.782724999999997 | loss 7.158271251472655\n",
      "Q values: tensor([1.6612, 1.5750, 1.5612], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 600 | reward 28.040075 | loss 14.017082601809165\n",
      "Q values: tensor([1.8081, 1.6911, 1.6930], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 700 | reward 43.622275 | loss 20.542589157594506\n",
      "Q values: tensor([2.6323, 2.4834, 2.4332], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 779 | reward 48.07912499999997 | loss 25.369488543399115\n",
      "Q values: tensor([1.9857, 1.9292, 1.9806], grad_fn=<SelectBackward>)\n",
      " 44%|████▍     | 44/100 [34:23<43:55, 47.07s/it]\n",
      " Episode 44 | step 100 | reward -2.929450000000001 | loss 0.7965551771796981\n",
      "Q values: tensor([1.2136, 1.1649, 1.1094], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 200 | reward -6.9512 | loss 1.680983189074368\n",
      "Q values: tensor([1.1916, 1.1624, 1.1470], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 300 | reward -4.998950000000002 | loss 2.2601247239272197\n",
      "Q values: tensor([1.3667, 1.3130, 1.3086], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 400 | reward 0.5089499999999986 | loss 3.1930444778893303\n",
      "Q values: tensor([1.4821, 1.4217, 1.4218], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 500 | reward 8.030599999999998 | loss 4.9872136154229025\n",
      "Q values: tensor([1.9360, 1.8485, 1.8236], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 600 | reward 13.956499999999997 | loss 6.18672472897245\n",
      "Q values: tensor([2.1585, 2.0674, 2.0505], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 700 | reward 28.776649999999982 | loss 11.629296924805686\n",
      "Q values: tensor([2.8923, 2.6822, 2.7163], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 759 | reward 38.39762499999998 | loss 23.388478433276333\n",
      "Q values: tensor([3.0988, 2.9572, 2.9146], grad_fn=<SelectBackward>)\n",
      " 45%|████▌     | 45/100 [35:11<43:11, 47.12s/it]\n",
      " Episode 45 | step 100 | reward -6.692350000000001 | loss 1.9398714726194157\n",
      "Q values: tensor([1.2397, 1.2964, 1.2454], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 200 | reward -15.460425 | loss 4.837044576944436\n",
      "Q values: tensor([1.8336, 1.8472, 1.7777], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 300 | reward -33.804875 | loss 13.740742730342163\n",
      "Q values: tensor([1.3990, 1.4897, 1.5096], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 400 | reward -52.75465 | loss 21.71287298380207\n",
      "Q values: tensor([1.5712, 1.6322, 1.7110], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 500 | reward -69.75732500000002 | loss 28.845610442099883\n",
      "Q values: tensor([1.7275, 1.8267, 1.8936], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 600 | reward -76.61542500000003 | loss 31.651618262033594\n",
      "Q values: tensor([2.2297, 2.3148, 2.2692], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 700 | reward -87.23112500000005 | loss 38.22173591752028\n",
      "Q values: tensor([2.5170, 2.6734, 2.6638], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 800 | reward -102.60170000000002 | loss 47.490861834639986\n",
      "Q values: tensor([1.8089, 1.8938, 1.9745], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 859 | reward -108.3817 | loss 55.703586888852556\n",
      "Q values: tensor([2.3561, 2.3226, 2.3729], grad_fn=<SelectBackward>)\n",
      " 46%|████▌     | 46/100 [35:58<42:31, 47.26s/it]\n",
      " Episode 46 | step 100 | reward -0.8726499999999999 | loss 0.3926533113177584\n",
      "Q values: tensor([1.1611, 1.2096, 1.2263], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 200 | reward -16.304925000000004 | loss 4.663766034923128\n",
      "Q values: tensor([0.9127, 1.0054, 1.0672], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 300 | reward -32.62035000000001 | loss 6.940078638785337\n",
      "Q values: tensor([1.0675, 1.1275, 1.2061], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 400 | reward -34.005675000000004 | loss 7.467831184631763\n",
      "Q values: tensor([1.4935, 1.5215, 1.5317], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 500 | reward -29.196749999999987 | loss 8.402341161795306\n",
      "Q values: tensor([1.8778, 1.8672, 1.8288], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 600 | reward -33.91044999999998 | loss 9.923762929377148\n",
      "Q values: tensor([1.2523, 1.2806, 1.3249], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 700 | reward -36.72899999999999 | loss 11.080976754400268\n",
      "Q values: tensor([1.7700, 1.7052, 1.7040], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 779 | reward -36.773774999999986 | loss 16.309396099387158\n",
      "Q values: tensor([1.9331, 1.9181, 1.9209], grad_fn=<SelectBackward>)\n",
      " 47%|████▋     | 47/100 [36:49<42:37, 48.25s/it]\n",
      " Episode 47 | step 100 | reward 4.9597 | loss 1.028791854073006\n",
      "Q values: tensor([1.0576, 1.0451, 1.0551], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 200 | reward 9.795900000000001 | loss 1.4941935059405296\n",
      "Q values: tensor([1.0455, 1.0061, 0.9991], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 300 | reward 9.777875 | loss 1.8196769694276895\n",
      "Q values: tensor([0.8765, 0.8603, 0.8690], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 400 | reward 10.295550000000002 | loss 2.273917312139133\n",
      "Q values: tensor([1.0821, 1.0642, 1.0560], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 500 | reward 22.459925000000005 | loss 5.619799397776774\n",
      "Q values: tensor([1.3620, 1.2860, 1.2319], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 600 | reward 31.791949999999986 | loss 8.279358714257\n",
      "Q values: tensor([1.8947, 1.7765, 1.8123], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 700 | reward 29.019025000000003 | loss 9.283134719567329\n",
      "Q values: tensor([1.6030, 1.5690, 1.5938], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 733 | reward 28.582150000000006 | loss 12.591458962387719\n",
      "Q values: tensor([1.7053, 1.6596, 1.6418], grad_fn=<SelectBackward>)\n",
      " 48%|████▊     | 48/100 [37:35<41:10, 47.51s/it]\n",
      " Episode 48 | step 100 | reward -1.3066749999999998 | loss 0.24873453713925642\n",
      "Q values: tensor([0.8948, 0.9041, 0.8916], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 200 | reward 5.0332 | loss 1.3918551651924247\n",
      "Q values: tensor([1.1301, 1.0924, 1.0800], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 300 | reward 22.638750000000012 | loss 7.125451248645874\n",
      "Q values: tensor([1.3992, 1.3147, 1.2782], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 400 | reward 40.06500000000002 | loss 13.991685835888262\n",
      "Q values: tensor([1.8283, 1.7348, 1.6774], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 500 | reward 49.78700000000002 | loss 16.101552693612263\n",
      "Q values: tensor([1.8049, 1.6965, 1.6936], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 600 | reward 74.21254999999996 | loss 29.160286434933965\n",
      "Q values: tensor([2.8938, 2.7840, 2.6034], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 700 | reward 100.08094999999993 | loss 45.72868926180732\n",
      "Q values: tensor([3.6315, 3.3118, 3.3523], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 743 | reward 104.02342499999995 | loss 62.727460796708925\n",
      "Q values: tensor([3.7988, 3.5994, 3.6964], grad_fn=<SelectBackward>)\n",
      " 49%|████▉     | 49/100 [38:22<40:15, 47.37s/it]\n",
      " Episode 49 | step 100 | reward 3.7003749999999984 | loss 0.23377513243599424\n",
      "Q values: tensor([1.6270, 1.6142, 1.5804], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 200 | reward 10.003424999999998 | loss 0.8812317225871311\n",
      "Q values: tensor([1.8465, 1.8231, 1.7814], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 300 | reward 23.955825000000004 | loss 3.260467219677082\n",
      "Q values: tensor([2.0904, 2.0405, 1.9861], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 400 | reward 43.572799999999994 | loss 10.52730039995032\n",
      "Q values: tensor([2.5489, 2.4691, 2.4001], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 500 | reward 64.78157499999998 | loss 18.507374829468574\n",
      "Q values: tensor([3.1710, 3.0422, 2.9783], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 600 | reward 79.74132499999992 | loss 23.441807250129898\n",
      "Q values: tensor([3.6780, 3.5215, 3.4559], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 700 | reward 105.07552499999994 | loss 33.741804459079106\n",
      "Q values: tensor([4.9317, 4.7040, 4.6334], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 741 | reward 115.85252499999994 | loss 64.96033871052128\n",
      "Q values: tensor([5.3301, 5.1729, 5.0238], grad_fn=<SelectBackward>)\n",
      " 50%|█████     | 50/100 [39:09<39:24, 47.30s/it]\n",
      " Episode 50 | step 100 | reward -4.315 | loss 1.7029170615605835\n",
      "Q values: tensor([1.8745, 1.8209, 1.7914], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 200 | reward -6.592524999999999 | loss 2.634361568271345\n",
      "Q values: tensor([2.1755, 2.1219, 2.1024], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 300 | reward 2.8401750000000026 | loss 4.4228686417660015\n",
      "Q values: tensor([3.0459, 2.8797, 2.8840], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 400 | reward 12.707125000000001 | loss 7.626036255434485\n",
      "Q values: tensor([3.2086, 3.0987, 3.0680], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 500 | reward 11.945649999999993 | loss 9.613914658851286\n",
      "Q values: tensor([3.1353, 3.1348, 3.0883], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 600 | reward 13.131324999999986 | loss 11.588344882595948\n",
      "Q values: tensor([3.6729, 3.6451, 3.5924], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 700 | reward 29.332424999999983 | loss 16.61714703614939\n",
      "Q values: tensor([4.3241, 4.2887, 4.2386], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 783 | reward 53.36457499999998 | loss 57.58841566173737\n",
      "Q values: tensor([6.0109, 5.6613, 5.6099], grad_fn=<SelectBackward>)\n",
      " 51%|█████     | 51/100 [39:59<39:23, 48.23s/it]\n",
      " Episode 51 | step 100 | reward 0.5593249999999999 | loss 0.6553707526039716\n",
      "Q values: tensor([1.8602, 1.8245, 1.8075], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 200 | reward 5.443249999999999 | loss 2.0912658687274757\n",
      "Q values: tensor([2.5974, 2.4553, 2.4796], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 300 | reward 13.886474999999994 | loss 4.713561739319573\n",
      "Q values: tensor([3.3743, 3.1383, 3.1767], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 400 | reward 5.6105749999999945 | loss 9.156176396862008\n",
      "Q values: tensor([3.3638, 3.2734, 3.2949], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 500 | reward -7.976750000000008 | loss 16.04941624577117\n",
      "Q values: tensor([3.4395, 3.5042, 3.4973], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 600 | reward -7.8342000000000045 | loss 19.055644536662328\n",
      "Q values: tensor([4.2476, 4.3018, 4.2292], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 700 | reward -13.866100000000007 | loss 24.323658312796795\n",
      "Q values: tensor([4.7964, 4.9447, 4.8179], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 762 | reward -23.259425000000007 | loss 52.28319891517884\n",
      "Q values: tensor([4.2574, 4.5563, 4.5161], grad_fn=<SelectBackward>)\n",
      " 52%|█████▏    | 52/100 [40:44<37:52, 47.34s/it]\n",
      " Episode 52 | step 100 | reward 2.3096499999999995 | loss 0.7077633808592196\n",
      "Q values: tensor([1.8457, 1.9437, 1.9173], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 200 | reward 2.2362999999999995 | loss 1.3997742168301244\n",
      "Q values: tensor([2.1588, 2.2351, 2.1987], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 300 | reward -7.813775 | loss 3.9458821235400734\n",
      "Q values: tensor([2.0696, 2.2048, 2.1729], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 400 | reward -29.35012499999999 | loss 12.471412039727298\n",
      "Q values: tensor([2.1315, 2.3148, 2.3300], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 500 | reward -41.98127499999998 | loss 17.396937640496915\n",
      "Q values: tensor([3.3346, 3.3658, 3.3539], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 600 | reward -56.36452499999998 | loss 23.82625602639412\n",
      "Q values: tensor([3.3559, 3.4869, 3.4448], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 700 | reward -85.11764999999998 | loss 40.461324639997045\n",
      "Q values: tensor([2.9584, 3.1824, 3.2854], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 765 | reward -103.45915000000001 | loss 65.91596188314755\n",
      "Q values: tensor([3.3353, 3.6173, 3.5443], grad_fn=<SelectBackward>)\n",
      " 53%|█████▎    | 53/100 [41:33<37:20, 47.68s/it]\n",
      " Episode 53 | step 100 | reward -7.053599999999998 | loss 0.8868865543528273\n",
      "Q values: tensor([1.3431, 1.4475, 1.4547], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 200 | reward -18.80435000000001 | loss 3.1619213790446565\n",
      "Q values: tensor([1.5142, 1.6417, 1.6348], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 300 | reward -42.898325000000035 | loss 11.042036472894452\n",
      "Q values: tensor([1.4666, 1.6659, 1.6735], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 400 | reward -61.910350000000044 | loss 16.16989949978428\n",
      "Q values: tensor([1.8969, 2.0290, 2.0319], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 500 | reward -74.6363 | loss 19.790424593207845\n",
      "Q values: tensor([2.5011, 2.5871, 2.6048], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 600 | reward -90.21629999999995 | loss 24.192933098198537\n",
      "Q values: tensor([2.0770, 2.2235, 2.2544], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 700 | reward -91.88847499999996 | loss 26.872978056097182\n",
      "Q values: tensor([2.9840, 2.9789, 2.9639], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 767 | reward -88.75897499999994 | loss 40.84629591042825\n",
      "Q values: tensor([3.4272, 3.3267, 3.1971], grad_fn=<SelectBackward>)\n",
      " 54%|█████▍    | 54/100 [42:21<36:33, 47.68s/it]\n",
      " Episode 54 | step 100 | reward -1.8343250000000018 | loss 0.2652190983693572\n",
      "Q values: tensor([1.0977, 1.1726, 1.1784], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 54 | step 200 | reward -2.560250000000001 | loss 0.882202045659028\n",
      "Q values: tensor([1.7022, 1.7625, 1.7352], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 54 | step 300 | reward -15.716150000000004 | loss 4.11309611225613\n",
      "Q values: tensor([1.4955, 1.5767, 1.5621], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 54 | step 400 | reward -44.844225 | loss 18.96722932007276\n",
      "Q values: tensor([1.2897, 1.4664, 1.5038], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 54 | step 500 | reward -73.42519999999999 | loss 30.155769960936937\n",
      "Q values: tensor([1.2620, 1.4574, 1.5353], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 54 | step 600 | reward -93.009025 | loss 34.4717168459905\n",
      "Q values: tensor([1.9744, 2.1814, 2.2474], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 54 | step 700 | reward -110.885125 | loss 39.00108603256933\n",
      "Q values: tensor([2.5882, 2.6912, 2.7162], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 54 | step 757 | reward -116.55297500000002 | loss 49.18479092620521\n",
      "Q values: tensor([2.2453, 2.3031, 2.4031], grad_fn=<SelectBackward>)\n",
      " 55%|█████▌    | 55/100 [43:09<35:51, 47.82s/it]\n",
      " Episode 55 | step 100 | reward -0.5019999999999999 | loss 1.1750314585415254\n",
      "Q values: tensor([0.7585, 0.8429, 0.9389], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 55 | step 200 | reward 0.9941749999999999 | loss 1.9582190544069817\n",
      "Q values: tensor([1.3090, 1.3584, 1.3682], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 55 | step 300 | reward -5.402075000000001 | loss 4.111738552489854\n",
      "Q values: tensor([1.3024, 1.3765, 1.4180], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 55 | step 400 | reward -23.50082500000001 | loss 9.525123952004563\n",
      "Q values: tensor([1.2551, 1.3941, 1.4658], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 55 | step 500 | reward -46.02960000000001 | loss 15.58307032383432\n",
      "Q values: tensor([1.6089, 1.7935, 1.8392], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 55 | step 600 | reward -74.40370000000001 | loss 22.664391477918755\n",
      "Q values: tensor([1.2378, 1.4716, 1.6033], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 55 | step 700 | reward -99.75895000000003 | loss 29.14410027416575\n",
      "Q values: tensor([2.0913, 2.2050, 2.2616], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 55 | step 764 | reward -111.85572500000004 | loss 38.557549146497834\n",
      "Q values: tensor([1.9857, 2.1232, 2.2304], grad_fn=<SelectBackward>)\n",
      " 56%|█████▌    | 56/100 [43:57<35:07, 47.89s/it]\n",
      " Episode 56 | step 100 | reward -8.168275 | loss 0.7350782896882606\n",
      "Q values: tensor([0.5273, 0.6248, 0.7412], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 56 | step 200 | reward -12.474725000000008 | loss 1.4211459614020328\n",
      "Q values: tensor([0.8160, 0.8978, 0.9682], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 56 | step 300 | reward 0.19727499999999576 | loss 7.344517421426815\n",
      "Q values: tensor([0.7302, 0.8227, 0.8504], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 56 | step 400 | reward 26.960874999999998 | loss 20.999342575386606\n",
      "Q values: tensor([1.1190, 1.1560, 1.0856], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 56 | step 500 | reward 64.41709999999998 | loss 43.91564525371058\n",
      "Q values: tensor([2.1687, 1.9968, 1.7707], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 56 | step 600 | reward 94.9057749999999 | loss 63.03643409711975\n",
      "Q values: tensor([2.0350, 1.9561, 1.8610], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 56 | step 700 | reward 115.50584999999991 | loss 71.17247413507926\n",
      "Q values: tensor([2.3532, 2.1694, 2.1355], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 56 | step 719 | reward 119.7408749999999 | loss 76.463134014057\n",
      "Q values: tensor([2.3456, 2.1416, 2.1496], grad_fn=<SelectBackward>)\n",
      " 57%|█████▋    | 57/100 [44:42<33:45, 47.10s/it]\n",
      " Episode 57 | step 100 | reward 2.54105 | loss 0.34719106272562783\n",
      "Q values: tensor([1.1221, 1.0917, 1.0587], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 200 | reward 1.445750000000001 | loss 0.7141641687930047\n",
      "Q values: tensor([1.1507, 1.1087, 1.1013], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 300 | reward -2.544699999999999 | loss 1.4208054197341795\n",
      "Q values: tensor([1.4026, 1.3378, 1.3242], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 400 | reward -10.235125000000005 | loss 3.183991432740143\n",
      "Q values: tensor([1.4285, 1.3747, 1.4185], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 500 | reward -12.787125000000005 | loss 4.479074156808522\n",
      "Q values: tensor([1.8797, 1.7871, 1.7868], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 600 | reward -11.538125000000008 | loss 5.579195498552465\n",
      "Q values: tensor([2.1214, 2.1078, 2.1445], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 700 | reward -23.803575000000013 | loss 10.546148487808935\n",
      "Q values: tensor([2.1742, 2.1890, 2.2058], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 800 | reward -43.529225000000004 | loss 16.955065034456496\n",
      "Q values: tensor([1.9856, 2.0784, 2.0774], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 57 | step 816 | reward -45.391775 | loss 22.816320086912313\n",
      "Q values: tensor([1.9979, 2.0892, 2.0780], grad_fn=<SelectBackward>)\n",
      " 58%|█████▊    | 58/100 [45:34<34:02, 48.62s/it]\n",
      " Episode 58 | step 100 | reward 3.69145 | loss 0.8170570718794465\n",
      "Q values: tensor([0.7931, 0.8873, 0.8522], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 58 | step 200 | reward 23.20757500000001 | loss 9.158878658132664\n",
      "Q values: tensor([1.3090, 1.3604, 1.2773], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 58 | step 300 | reward 43.98125 | loss 17.064185818578665\n",
      "Q values: tensor([1.4206, 1.4395, 1.3368], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 58 | step 400 | reward 65.95519999999998 | loss 26.56997045717941\n",
      "Q values: tensor([1.7827, 1.7245, 1.6646], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 58 | step 500 | reward 96.08299999999996 | loss 40.14291592907972\n",
      "Q values: tensor([1.9511, 1.9145, 1.8487], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 58 | step 600 | reward 118.13932499999991 | loss 52.6412958656943\n",
      "Q values: tensor([2.6098, 2.5499, 2.5343], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 58 | step 700 | reward 138.25419999999994 | loss 58.59609072262498\n",
      "Q values: tensor([2.7398, 2.7121, 2.5559], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 58 | step 757 | reward 146.18979999999993 | loss 71.04809064939724\n",
      "Q values: tensor([3.2990, 3.1776, 3.1298], grad_fn=<SelectBackward>)\n",
      " 59%|█████▉    | 59/100 [46:19<32:31, 47.59s/it]\n",
      " Episode 59 | step 100 | reward 2.584399999999998 | loss 0.3279174621476528\n",
      "Q values: tensor([1.7009, 1.6645, 1.6577], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 59 | step 200 | reward -1.8287000000000018 | loss 1.8794780636213915\n",
      "Q values: tensor([1.7706, 1.7515, 1.7450], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 59 | step 300 | reward -1.9586500000000044 | loss 2.720244945480218\n",
      "Q values: tensor([2.1283, 2.1157, 2.0621], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 59 | step 400 | reward -2.5116750000000043 | loss 3.752638939606655\n",
      "Q values: tensor([2.0250, 1.9922, 1.9997], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 59 | step 500 | reward -6.045625000000006 | loss 5.387366246817251\n",
      "Q values: tensor([2.4778, 2.4447, 2.4243], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 59 | step 600 | reward -8.095325000000008 | loss 6.571626244275208\n",
      "Q values: tensor([2.6905, 2.6633, 2.6066], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 59 | step 700 | reward -3.3815250000000088 | loss 8.79688909668495\n",
      "Q values: tensor([2.9675, 2.9325, 2.9115], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 59 | step 777 | reward -3.0395000000000083 | loss 24.067653277381638\n",
      "Q values: tensor([3.3465, 3.4434, 3.3964], grad_fn=<SelectBackward>)\n",
      " 60%|██████    | 60/100 [46:58<29:53, 44.83s/it]\n",
      " Episode 60 | step 100 | reward -0.2662749999999996 | loss 0.4517946627420457\n",
      "Q values: tensor([1.3369, 1.3937, 1.3938], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 60 | step 200 | reward -11.759250000000003 | loss 2.572534937373362\n",
      "Q values: tensor([1.5054, 1.5862, 1.6507], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 60 | step 300 | reward -32.535925000000006 | loss 8.258494250760933\n",
      "Q values: tensor([1.5225, 1.6264, 1.7050], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 60 | step 400 | reward -50.880825000000016 | loss 12.103690047447245\n",
      "Q values: tensor([1.5081, 1.6347, 1.6709], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 60 | step 500 | reward -63.807700000000025 | loss 14.019339147742258\n",
      "Q values: tensor([1.6832, 1.8217, 1.8534], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 60 | step 600 | reward -72.908825 | loss 15.864454319717794\n",
      "Q values: tensor([1.6049, 1.6639, 1.7161], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 60 | step 700 | reward -78.93355000000003 | loss 17.88562591355131\n",
      "Q values: tensor([2.1031, 2.1802, 2.1993], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 60 | step 786 | reward -91.733675 | loss 27.46170187067392\n",
      "Q values: tensor([1.6755, 1.8209, 1.8832], grad_fn=<SelectBackward>)\n",
      " 61%|██████    | 61/100 [47:33<27:17, 42.00s/it]\n",
      " Episode 61 | step 100 | reward 2.0565000000000007 | loss 0.07487891432257143\n",
      "Q values: tensor([0.9949, 0.9936, 0.9822], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 61 | step 200 | reward 0.8504249999999998 | loss 0.4125436362006134\n",
      "Q values: tensor([1.0829, 1.0774, 1.0636], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 61 | step 300 | reward -0.8637999999999999 | loss 0.7518595358502616\n",
      "Q values: tensor([1.2007, 1.1784, 1.1445], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 61 | step 400 | reward 8.032774999999999 | loss 2.0042116564412744\n",
      "Q values: tensor([1.4735, 1.4228, 1.3903], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 61 | step 500 | reward 20.677425000000003 | loss 4.3282059620881945\n",
      "Q values: tensor([1.8074, 1.7474, 1.7008], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 61 | step 600 | reward 38.412025 | loss 10.356313945705441\n",
      "Q values: tensor([2.4282, 2.3122, 2.2566], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 61 | step 700 | reward 58.7351 | loss 19.44245308978805\n",
      "Q values: tensor([2.7726, 2.6851, 2.6825], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 61 | step 734 | reward 70.51009999999998 | loss 36.650850105303505\n",
      "Q values: tensor([3.6910, 3.4894, 3.3837], grad_fn=<SelectBackward>)\n",
      " 62%|██████▏   | 62/100 [48:09<25:27, 40.20s/it]\n",
      " Episode 62 | step 100 | reward -4.977549999999999 | loss 0.9828536029228871\n",
      "Q values: tensor([1.3229, 1.3165, 1.2702], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 62 | step 200 | reward -13.035324999999997 | loss 2.859353260170735\n",
      "Q values: tensor([1.4193, 1.4307, 1.3876], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 62 | step 300 | reward -13.727399999999989 | loss 3.100990590358677\n",
      "Q values: tensor([1.5335, 1.5551, 1.5296], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 62 | step 400 | reward -6.454149999999989 | loss 3.869557562237418\n",
      "Q values: tensor([2.0853, 2.0439, 1.9836], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 62 | step 500 | reward -2.6693249999999873 | loss 5.598085683070309\n",
      "Q values: tensor([2.1274, 2.1606, 2.1553], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 62 | step 600 | reward -15.415699999999987 | loss 9.760476476736727\n",
      "Q values: tensor([2.4079, 2.3619, 2.4293], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 62 | step 700 | reward -24.35772499999998 | loss 13.401171378560463\n",
      "Q values: tensor([2.7877, 2.7339, 2.8005], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 62 | step 744 | reward -31.336324999999984 | loss 24.446212928720485\n",
      "Q values: tensor([2.5001, 2.5509, 2.6274], grad_fn=<SelectBackward>)\n",
      " 63%|██████▎   | 63/100 [48:46<24:08, 39.14s/it]\n",
      " Episode 63 | step 100 | reward -0.9206250000000002 | loss 0.10747708842425041\n",
      "Q values: tensor([1.0127, 1.0671, 1.0837], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 63 | step 200 | reward 4.971374999999998 | loss 1.087856964915705\n",
      "Q values: tensor([1.1783, 1.2204, 1.2045], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 63 | step 300 | reward 14.822 | loss 2.7754159606374067\n",
      "Q values: tensor([1.4644, 1.4737, 1.4435], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 63 | step 400 | reward 10.978199999999998 | loss 3.592952312611647\n",
      "Q values: tensor([1.4045, 1.4638, 1.4435], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 63 | step 500 | reward 6.571699999999998 | loss 4.605769666469019\n",
      "Q values: tensor([1.5416, 1.5963, 1.5905], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 63 | step 600 | reward 22.476699999999997 | loss 8.73742535448007\n",
      "Q values: tensor([2.1708, 2.1519, 2.0442], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 63 | step 700 | reward 46.34537500000001 | loss 19.691881077301947\n",
      "Q values: tensor([3.3856, 3.2337, 3.1806], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 63 | step 763 | reward 62.18310000000001 | loss 36.71752857215135\n",
      "Q values: tensor([3.4546, 3.4877, 3.3650], grad_fn=<SelectBackward>)\n",
      " 64%|██████▍   | 64/100 [49:25<23:25, 39.05s/it]\n",
      " Episode 64 | step 100 | reward 0.9187999999999996 | loss 0.1721761515246385\n",
      "Q values: tensor([1.2934, 1.2913, 1.2374], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 64 | step 200 | reward 6.4415499999999986 | loss 0.6917593027392428\n",
      "Q values: tensor([1.6754, 1.6510, 1.5985], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 64 | step 300 | reward 8.58545 | loss 1.0993338471007803\n",
      "Q values: tensor([1.6606, 1.6760, 1.6368], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 64 | step 400 | reward 3.884425000000001 | loss 2.4503614042260153\n",
      "Q values: tensor([2.0168, 2.0172, 1.9851], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 64 | step 500 | reward -7.233124999999998 | loss 6.042262567994413\n",
      "Q values: tensor([1.6338, 1.6771, 1.6811], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 64 | step 600 | reward -14.088274999999998 | loss 8.324610334618711\n",
      "Q values: tensor([2.2075, 2.1893, 2.1707], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 64 | step 700 | reward -19.057600000000004 | loss 11.838343704422897\n",
      "Q values: tensor([2.8252, 2.9053, 2.8911], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 64 | step 751 | reward -27.665675 | loss 25.58565450425138\n",
      "Q values: tensor([2.8739, 2.9255, 2.9525], grad_fn=<SelectBackward>)\n",
      " 65%|██████▌   | 65/100 [50:04<22:51, 39.17s/it]\n",
      " Episode 65 | step 100 | reward -4.0032749999999995 | loss 0.5311515153754272\n",
      "Q values: tensor([0.9951, 1.0206, 1.0384], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 65 | step 200 | reward -13.695524999999998 | loss 2.343200192268583\n",
      "Q values: tensor([1.1444, 1.1713, 1.1912], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 65 | step 300 | reward -31.16397499999999 | loss 7.1498481504661555\n",
      "Q values: tensor([0.9456, 0.9912, 1.0504], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 65 | step 400 | reward -48.608775000000016 | loss 11.770439009160341\n",
      "Q values: tensor([1.1177, 1.1594, 1.2328], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 65 | step 500 | reward -56.047924999999985 | loss 13.291928557495353\n",
      "Q values: tensor([1.7980, 1.8286, 1.8582], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 65 | step 600 | reward -80.32780000000001 | loss 20.52568796961897\n",
      "Q values: tensor([1.5322, 1.6403, 1.7842], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 65 | step 700 | reward -112.34795000000003 | loss 30.237223718593683\n",
      "Q values: tensor([1.3952, 1.4919, 1.7457], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 65 | step 767 | reward -130.89114999999998 | loss 36.55268372638609\n",
      "Q values: tensor([1.3609, 1.4236, 1.7053], grad_fn=<SelectBackward>)\n",
      " 66%|██████▌   | 66/100 [50:44<22:21, 39.45s/it]\n",
      " Episode 66 | step 100 | reward 2.9133 | loss 1.1416504635571982\n",
      "Q values: tensor([0.8161, 0.8329, 0.8853], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 66 | step 200 | reward 18.223574999999997 | loss 5.928639819271666\n",
      "Q values: tensor([1.0617, 1.0536, 1.0379], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 66 | step 300 | reward 40.74424999999998 | loss 14.993315272699874\n",
      "Q values: tensor([1.4144, 1.3420, 1.3401], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 66 | step 400 | reward 55.89034999999998 | loss 20.733624289029095\n",
      "Q values: tensor([1.7295, 1.5939, 1.5797], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 66 | step 500 | reward 71.2518 | loss 27.01596132188901\n",
      "Q values: tensor([2.1244, 1.9753, 1.9081], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 66 | step 600 | reward 88.60232499999998 | loss 34.46692636901902\n",
      "Q values: tensor([2.7046, 2.6072, 2.5513], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 66 | step 700 | reward 120.897825 | loss 56.81873473237614\n",
      "Q values: tensor([3.5102, 3.3033, 3.3618], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 66 | step 745 | reward 136.06147499999994 | loss 91.51436325713406\n",
      "Q values: tensor([5.2429, 4.8274, 4.9018], grad_fn=<SelectBackward>)\n",
      " 67%|██████▋   | 67/100 [51:23<21:36, 39.29s/it]\n",
      " Episode 67 | step 100 | reward 2.892250000000001 | loss 1.0100694787841036\n",
      "Q values: tensor([1.1818, 1.1259, 1.1576], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 67 | step 200 | reward -1.3316499999999976 | loss 2.6328430014048223\n",
      "Q values: tensor([1.4306, 1.3744, 1.3985], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 67 | step 300 | reward -14.921124999999998 | loss 6.586736887680345\n",
      "Q values: tensor([1.3898, 1.3584, 1.4438], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 67 | step 400 | reward -9.571424999999993 | loss 7.97433916835611\n",
      "Q values: tensor([2.2215, 2.1681, 2.1776], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 67 | step 500 | reward -2.0459249999999956 | loss 10.691703459048213\n",
      "Q values: tensor([2.5951, 2.5656, 2.5448], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 67 | step 600 | reward 8.646150000000006 | loss 13.936259539445373\n",
      "Q values: tensor([2.9377, 2.9282, 2.8643], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 67 | step 700 | reward 24.959975000000004 | loss 18.811249394181132\n",
      "Q values: tensor([3.4431, 3.3782, 3.3906], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 67 | step 769 | reward 41.387250000000016 | loss 39.105690467107664\n",
      "Q values: tensor([4.0598, 4.0138, 3.9357], grad_fn=<SelectBackward>)\n",
      " 68%|██████▊   | 68/100 [52:02<20:57, 39.29s/it]\n",
      " Episode 68 | step 100 | reward 2.566599999999998 | loss 0.6112082571469273\n",
      "Q values: tensor([1.3047, 1.2893, 1.2315], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 68 | step 200 | reward 11.687599999999994 | loss 3.306978949253349\n",
      "Q values: tensor([2.1090, 2.0469, 1.9262], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 68 | step 300 | reward 16.06909999999999 | loss 5.5039699958458215\n",
      "Q values: tensor([1.6905, 1.6911, 1.6732], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 68 | step 400 | reward 19.96489999999999 | loss 7.26705556510997\n",
      "Q values: tensor([2.3472, 2.3221, 2.2606], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 68 | step 500 | reward 36.475149999999985 | loss 12.759216752472597\n",
      "Q values: tensor([2.5379, 2.5377, 2.4914], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 68 | step 600 | reward 58.91464999999998 | loss 21.47561509447219\n",
      "Q values: tensor([4.0830, 4.0249, 3.9008], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 68 | step 700 | reward 73.32870000000004 | loss 27.790961076609804\n",
      "Q values: tensor([4.4015, 4.3660, 4.2342], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 68 | step 767 | reward 81.84460000000001 | loss 45.513060858215525\n",
      "Q values: tensor([4.0674, 4.0488, 3.9122], grad_fn=<SelectBackward>)\n",
      " 69%|██████▉   | 69/100 [52:43<20:26, 39.57s/it]\n",
      " Episode 69 | step 100 | reward -0.8200750000000001 | loss 0.8374257230098578\n",
      "Q values: tensor([1.0151, 1.0439, 1.0834], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 69 | step 200 | reward -4.550850000000001 | loss 1.8196092270503073\n",
      "Q values: tensor([1.5115, 1.4964, 1.4828], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 69 | step 300 | reward -13.387750000000002 | loss 5.023572504158153\n",
      "Q values: tensor([1.4851, 1.5137, 1.5445], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 69 | step 400 | reward -24.933300000000013 | loss 7.813595082730615\n",
      "Q values: tensor([1.3805, 1.4396, 1.5367], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 69 | step 500 | reward -24.677050000000005 | loss 10.096340285603844\n",
      "Q values: tensor([2.4119, 2.3889, 2.3358], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 69 | step 600 | reward -18.135225000000016 | loss 13.183884317299714\n",
      "Q values: tensor([2.8632, 2.8891, 2.7629], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 69 | step 700 | reward -4.508850000000013 | loss 18.067481646477745\n",
      "Q values: tensor([3.3193, 3.4160, 3.2286], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 69 | step 742 | reward 1.546099999999987 | loss 34.526251779415134\n",
      "Q values: tensor([3.8952, 3.9609, 3.8281], grad_fn=<SelectBackward>)\n",
      " 70%|███████   | 70/100 [53:21<19:35, 39.19s/it]\n",
      " Episode 70 | step 100 | reward -1.1385750000000001 | loss 0.650295324572653\n",
      "Q values: tensor([1.2931, 1.2594, 1.2661], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 70 | step 200 | reward 2.9069249999999993 | loss 1.3938437218071158\n",
      "Q values: tensor([1.8833, 1.8141, 1.7850], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 70 | step 300 | reward 13.554474999999996 | loss 4.661697901151911\n",
      "Q values: tensor([2.3119, 2.2260, 2.1584], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 70 | step 400 | reward 30.266200000000005 | loss 10.743860327852872\n",
      "Q values: tensor([2.7895, 2.6867, 2.6221], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 70 | step 500 | reward 33.52075000000002 | loss 14.145491235426107\n",
      "Q values: tensor([2.5922, 2.5622, 2.5401], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 70 | step 600 | reward 28.38790000000003 | loss 17.446058746253357\n",
      "Q values: tensor([2.9187, 2.9728, 2.9106], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 70 | step 700 | reward 15.274550000000017 | loss 24.24900540593785\n",
      "Q values: tensor([2.7553, 2.8767, 2.8549], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 70 | step 799 | reward 14.113750000000014 | loss 44.58398166994425\n",
      "Q values: tensor([3.5073, 3.6227, 3.4822], grad_fn=<SelectBackward>)\n",
      " 71%|███████   | 71/100 [54:05<19:33, 40.48s/it]\n",
      " Episode 71 | step 100 | reward 1.3241000000000003 | loss 0.8885692245487498\n",
      "Q values: tensor([1.1749, 1.1973, 1.1802], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 71 | step 200 | reward 7.404100000000001 | loss 2.5922724312270056\n",
      "Q values: tensor([1.7455, 1.7096, 1.6365], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 71 | step 300 | reward 11.056200000000008 | loss 3.5972997176888875\n",
      "Q values: tensor([1.6110, 1.6231, 1.5677], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 71 | step 400 | reward 10.21945000000001 | loss 4.647231452644068\n",
      "Q values: tensor([1.8814, 1.9060, 1.8640], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 71 | step 500 | reward 2.756175000000008 | loss 7.839818627472713\n",
      "Q values: tensor([2.1318, 2.1850, 2.2181], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 71 | step 600 | reward -12.32907499999999 | loss 12.233727658299475\n",
      "Q values: tensor([1.5045, 1.6756, 1.7987], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 71 | step 700 | reward -18.610000000000003 | loss 16.207129545660354\n",
      "Q values: tensor([2.3719, 2.4200, 2.4547], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 71 | step 747 | reward -18.459824999999995 | loss 32.37201763563782\n",
      "Q values: tensor([3.5713, 3.4661, 3.4977], grad_fn=<SelectBackward>)\n",
      " 72%|███████▏  | 72/100 [54:41<18:19, 39.28s/it]\n",
      " Episode 72 | step 100 | reward 4.23415 | loss 0.7965922505584331\n",
      "Q values: tensor([1.0203, 0.9934, 0.9779], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 72 | step 200 | reward 20.745525 | loss 5.093745810548398\n",
      "Q values: tensor([1.3041, 1.2695, 1.2101], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 72 | step 300 | reward 37.23045 | loss 10.941636530100595\n",
      "Q values: tensor([1.7890, 1.7534, 1.6581], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 72 | step 400 | reward 47.37794999999999 | loss 13.347994169856975\n",
      "Q values: tensor([1.6548, 1.6359, 1.5891], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 72 | step 500 | reward 60.74487500000002 | loss 16.68343223809643\n",
      "Q values: tensor([2.8881, 2.8228, 2.7383], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 72 | step 600 | reward 81.79545000000003 | loss 24.486724820059408\n",
      "Q values: tensor([2.7820, 2.8220, 2.7132], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 72 | step 700 | reward 108.36712500000002 | loss 39.57662963479788\n",
      "Q values: tensor([3.8150, 3.9642, 3.6841], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 72 | step 739 | reward 122.606375 | loss 59.71997847134162\n",
      "Q values: tensor([3.8762, 3.9232, 3.6103], grad_fn=<SelectBackward>)\n",
      " 73%|███████▎  | 73/100 [55:16<17:09, 38.15s/it]\n",
      " Episode 73 | step 100 | reward 3.968625000000002 | loss 1.0626148174305854\n",
      "Q values: tensor([1.9878, 1.9348, 1.8268], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 73 | step 200 | reward -0.9262999999999972 | loss 3.718013174441012\n",
      "Q values: tensor([2.3432, 2.3257, 2.2595], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 73 | step 300 | reward -21.109049999999996 | loss 14.445617147092719\n",
      "Q values: tensor([2.2600, 2.2428, 2.2103], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 73 | step 400 | reward -46.35022500000001 | loss 29.715453118102573\n",
      "Q values: tensor([2.3075, 2.3059, 2.3612], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 73 | step 500 | reward -72.52005000000003 | loss 41.933433347143705\n",
      "Q values: tensor([3.0327, 3.1007, 3.2565], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 73 | step 600 | reward -94.536825 | loss 49.11034835240059\n",
      "Q values: tensor([3.8779, 3.8775, 4.1365], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 73 | step 700 | reward -108.51842500000004 | loss 52.94359245479643\n",
      "Q values: tensor([4.3275, 4.4278, 4.5812], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 73 | step 712 | reward -110.76467500000005 | loss 79.2821269997097\n",
      "Q values: tensor([4.6335, 4.7623, 4.8636], grad_fn=<SelectBackward>)\n",
      " 74%|███████▍  | 74/100 [55:52<16:08, 37.26s/it]\n",
      " Episode 74 | step 100 | reward -2.8882749999999997 | loss 0.43738317515040137\n",
      "Q values: tensor([1.1846, 1.1330, 1.1101], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 200 | reward -7.501625 | loss 1.5444280703563464\n",
      "Q values: tensor([1.4850, 1.4892, 1.5044], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 300 | reward -22.87737500000001 | loss 7.361791126966139\n",
      "Q values: tensor([1.3804, 1.4188, 1.4148], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 400 | reward -39.572525000000034 | loss 13.063028785876625\n",
      "Q values: tensor([1.8304, 1.8767, 1.9248], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 500 | reward -48.36712500000002 | loss 15.913076864223925\n",
      "Q values: tensor([2.2032, 2.2778, 2.3070], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 600 | reward -64.21587500000003 | loss 21.015966517169957\n",
      "Q values: tensor([1.8291, 1.9492, 2.0039], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 700 | reward -86.60582499999988 | loss 28.58807685316738\n",
      "Q values: tensor([1.5084, 1.5626, 1.7816], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 800 | reward -97.42459999999991 | loss 33.37980325855969\n",
      "Q values: tensor([2.4413, 2.5272, 2.6522], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 74 | step 813 | reward -98.32972499999991 | loss 42.267506831926454\n",
      "Q values: tensor([2.5053, 2.6002, 2.6817], grad_fn=<SelectBackward>)\n",
      " 75%|███████▌  | 75/100 [56:34<16:07, 38.70s/it]\n",
      " Episode 75 | step 100 | reward 5.12745 | loss 2.6832778336809042\n",
      "Q values: tensor([1.2302, 1.2638, 1.3810], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 75 | step 200 | reward 11.728350000000002 | loss 5.017083080040493\n",
      "Q values: tensor([1.6435, 1.6884, 1.7744], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 75 | step 300 | reward 20.500499999999995 | loss 7.5121748346155215\n",
      "Q values: tensor([1.8139, 1.8337, 1.8656], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 75 | step 400 | reward 12.147049999999998 | loss 10.849658578912127\n",
      "Q values: tensor([1.6723, 1.6794, 1.7380], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 75 | step 500 | reward 2.1182749999999975 | loss 14.965075479170778\n",
      "Q values: tensor([1.9420, 1.9651, 2.0390], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 75 | step 600 | reward -11.498625 | loss 18.257027294613025\n",
      "Q values: tensor([2.3201, 2.3315, 2.3761], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 75 | step 700 | reward -16.82325000000001 | loss 22.213061093240313\n",
      "Q values: tensor([2.8580, 2.9083, 2.9514], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 75 | step 788 | reward -33.853600000000014 | loss 40.8397348917173\n",
      "Q values: tensor([2.9671, 3.2342, 3.3242], grad_fn=<SelectBackward>)\n",
      " 76%|███████▌  | 76/100 [57:14<15:40, 39.20s/it]\n",
      " Episode 76 | step 100 | reward -0.3523000000000001 | loss 0.41665629249166614\n",
      "Q values: tensor([0.9499, 0.9792, 1.0080], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 76 | step 200 | reward -5.547225000000001 | loss 1.558035171301185\n",
      "Q values: tensor([0.9484, 0.9814, 1.0260], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 76 | step 300 | reward -10.899525 | loss 2.437736107897706\n",
      "Q values: tensor([1.0205, 1.0648, 1.1104], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 76 | step 400 | reward -6.842825000000005 | loss 3.9287125470481783\n",
      "Q values: tensor([1.5456, 1.6169, 1.6118], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 76 | step 500 | reward -3.070700000000005 | loss 5.258727634399804\n",
      "Q values: tensor([1.9450, 2.0103, 1.9832], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 76 | step 600 | reward -4.251650000000004 | loss 6.517807437876002\n",
      "Q values: tensor([1.3469, 1.3758, 1.4258], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 76 | step 700 | reward 4.464899999999997 | loss 9.486895858581022\n",
      "Q values: tensor([2.4529, 2.4764, 2.3678], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 76 | step 783 | reward 11.006875000000003 | loss 22.965466669285718\n",
      "Q values: tensor([3.4660, 3.4443, 3.3385], grad_fn=<SelectBackward>)\n",
      " 77%|███████▋  | 77/100 [57:55<15:15, 39.79s/it]\n",
      " Episode 77 | step 100 | reward 3.5988749999999983 | loss 1.0452063833772627\n",
      "Q values: tensor([1.0451, 0.9476, 0.9457], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 77 | step 200 | reward 1.9764999999999995 | loss 1.7259307185099715\n",
      "Q values: tensor([1.1460, 1.0776, 1.0950], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 77 | step 300 | reward -8.241425 | loss 5.089356358375326\n",
      "Q values: tensor([1.2341, 1.2418, 1.2593], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 77 | step 400 | reward -25.62140000000001 | loss 12.104375991452244\n",
      "Q values: tensor([1.2872, 1.3636, 1.4116], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 77 | step 500 | reward -54.5741 | loss 20.821891525707898\n",
      "Q values: tensor([0.8884, 1.0526, 1.1847], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 77 | step 600 | reward -69.92752499999999 | loss 23.87844808362702\n",
      "Q values: tensor([1.9690, 2.0487, 2.0967], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 77 | step 700 | reward -84.90312499999999 | loss 27.81219722652027\n",
      "Q values: tensor([1.7296, 1.9348, 2.0049], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 77 | step 764 | reward -97.14677499999999 | loss 38.93268304956878\n",
      "Q values: tensor([2.1200, 2.1964, 2.2984], grad_fn=<SelectBackward>)\n",
      " 78%|███████▊  | 78/100 [58:35<14:38, 39.91s/it]\n",
      " Episode 78 | step 100 | reward 1.6977 | loss 0.5266748208359786\n",
      "Q values: tensor([0.8089, 0.8564, 0.9166], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 78 | step 200 | reward -6.4891 | loss 2.072252092729485\n",
      "Q values: tensor([0.6850, 0.7377, 0.7850], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 78 | step 300 | reward -10.180249999999992 | loss 3.9871272732754903\n",
      "Q values: tensor([0.8349, 0.8482, 0.8618], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 78 | step 400 | reward 3.5653250000000076 | loss 7.229972163730821\n",
      "Q values: tensor([1.0571, 1.0287, 1.0264], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 78 | step 500 | reward 14.916425000000006 | loss 10.645165239767408\n",
      "Q values: tensor([1.1104, 1.0636, 1.0913], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 78 | step 600 | reward 31.182475000000004 | loss 15.755686699013637\n",
      "Q values: tensor([1.8746, 1.8445, 1.7540], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 78 | step 700 | reward 33.594599999999986 | loss 17.522312599454523\n",
      "Q values: tensor([1.8129, 1.8081, 1.8071], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 78 | step 790 | reward 24.67555 | loss 24.63595960105536\n",
      "Q values: tensor([1.1243, 1.2387, 1.3294], grad_fn=<SelectBackward>)\n",
      " 79%|███████▉  | 79/100 [59:16<13:59, 39.96s/it]\n",
      " Episode 79 | step 100 | reward 5.784024999999998 | loss 0.9761790377695547\n",
      "Q values: tensor([1.0062, 1.0064, 0.9992], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 79 | step 200 | reward 20.124874999999996 | loss 5.301010439469735\n",
      "Q values: tensor([1.1525, 1.1023, 1.0702], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 79 | step 300 | reward 44.57427499999997 | loss 14.282996881291922\n",
      "Q values: tensor([1.5071, 1.4609, 1.3419], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 79 | step 400 | reward 76.55447499999995 | loss 31.874540120879203\n",
      "Q values: tensor([1.8707, 1.7722, 1.6236], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 79 | step 500 | reward 94.116975 | loss 40.9067690335026\n",
      "Q values: tensor([1.9808, 1.8683, 1.8759], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 79 | step 600 | reward 104.69815000000004 | loss 44.913702582976214\n",
      "Q values: tensor([2.3074, 2.1774, 2.1580], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 79 | step 700 | reward 124.77920000000003 | loss 54.34678072491744\n",
      "Q values: tensor([3.5137, 3.3444, 3.2888], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 79 | step 717 | reward 128.75725000000003 | loss 71.56459696274334\n",
      "Q values: tensor([4.0973, 3.8425, 3.7857], grad_fn=<SelectBackward>)\n",
      " 80%|████████  | 80/100 [59:53<13:06, 39.35s/it]\n",
      " Episode 80 | step 100 | reward 5.200975 | loss 1.7286490448482255\n",
      "Q values: tensor([1.5146, 1.4095, 1.3310], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 80 | step 200 | reward 5.572900000000003 | loss 2.481507793234769\n",
      "Q values: tensor([1.5204, 1.4379, 1.3874], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 80 | step 300 | reward 2.726550000000005 | loss 3.695690756888226\n",
      "Q values: tensor([1.2421, 1.2043, 1.2308], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 80 | step 400 | reward -1.5607749999999947 | loss 5.753377003333199\n",
      "Q values: tensor([1.6228, 1.5904, 1.5872], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 80 | step 500 | reward 2.9891250000000054 | loss 7.092108109881465\n",
      "Q values: tensor([2.5088, 2.3976, 2.3448], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 80 | step 600 | reward 4.662325000000004 | loss 9.149733309226704\n",
      "Q values: tensor([2.2167, 2.2360, 2.2420], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 80 | step 700 | reward -0.8173749999999955 | loss 12.592339303439203\n",
      "Q values: tensor([2.4105, 2.4504, 2.5307], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 80 | step 748 | reward -4.517524999999996 | loss 21.838471100592997\n",
      "Q values: tensor([2.4174, 2.4616, 2.5333], grad_fn=<SelectBackward>)\n",
      " 81%|████████  | 81/100 [1:00:31<12:17, 38.84s/it]\n",
      " Episode 81 | step 100 | reward 1.2423 | loss 0.2854578507723886\n",
      "Q values: tensor([1.2525, 1.2481, 1.2207], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 81 | step 200 | reward -3.624375 | loss 1.6996731877417233\n",
      "Q values: tensor([1.1366, 1.1397, 1.1561], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 81 | step 300 | reward -10.660824999999997 | loss 3.0786716531973894\n",
      "Q values: tensor([1.2401, 1.2224, 1.2407], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 81 | step 400 | reward -13.633499999999996 | loss 4.401721384521707\n",
      "Q values: tensor([1.3944, 1.4004, 1.4406], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 81 | step 500 | reward -21.456874999999997 | loss 7.366951206806519\n",
      "Q values: tensor([1.2335, 1.3000, 1.3595], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 81 | step 600 | reward -33.91777499999999 | loss 10.268375048619149\n",
      "Q values: tensor([1.9686, 1.9463, 1.9972], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 81 | step 700 | reward -46.580899999999986 | loss 14.566645894754132\n",
      "Q values: tensor([1.9285, 2.0628, 2.0218], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 81 | step 744 | reward -54.59159999999999 | loss 23.26832392600842\n",
      "Q values: tensor([1.8257, 1.9790, 2.0240], grad_fn=<SelectBackward>)\n",
      " 82%|████████▏ | 82/100 [1:01:08<11:30, 38.36s/it]\n",
      " Episode 82 | step 100 | reward 6.6788 | loss 1.5137605762208892\n",
      "Q values: tensor([1.2378, 1.2947, 1.2589], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 82 | step 200 | reward 8.969074999999988 | loss 2.272199588507661\n",
      "Q values: tensor([1.0941, 1.1509, 1.1357], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 82 | step 300 | reward 17.86309999999999 | loss 4.669311729535892\n",
      "Q values: tensor([1.5590, 1.5837, 1.5048], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 82 | step 400 | reward 35.87319999999997 | loss 10.505068411700066\n",
      "Q values: tensor([2.0328, 2.0203, 1.9354], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 82 | step 500 | reward 57.093599999999995 | loss 18.8620244469691\n",
      "Q values: tensor([1.9616, 1.9670, 1.9323], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 82 | step 600 | reward 82.84265000000003 | loss 31.435998302927842\n",
      "Q values: tensor([2.9351, 2.7631, 2.7585], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 82 | step 700 | reward 123.05205000000004 | loss 63.963151389587225\n",
      "Q values: tensor([4.4774, 4.3158, 4.1111], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 82 | step 736 | reward 140.39755 | loss 94.65141688776181\n",
      "Q values: tensor([4.8630, 4.6769, 4.3967], grad_fn=<SelectBackward>)\n",
      " 83%|████████▎ | 83/100 [1:01:46<10:47, 38.07s/it]\n",
      " Episode 83 | step 100 | reward -3.9813999999999994 | loss 1.9292926228717988\n",
      "Q values: tensor([1.4636, 1.4328, 1.3229], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 200 | reward -14.4066 | loss 6.369076416243004\n",
      "Q values: tensor([1.2885, 1.3148, 1.2868], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 300 | reward -28.013374999999993 | loss 11.15109090512074\n",
      "Q values: tensor([1.3985, 1.4499, 1.4311], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 400 | reward -36.569024999999996 | loss 14.822532784951619\n",
      "Q values: tensor([2.1804, 2.1864, 2.1386], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 500 | reward -58.14857499999999 | loss 26.971253374985537\n",
      "Q values: tensor([1.9642, 2.0454, 2.0557], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 600 | reward -79.660825 | loss 36.32710780883056\n",
      "Q values: tensor([1.8477, 1.9518, 2.0341], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 700 | reward -94.734525 | loss 42.80381083988553\n",
      "Q values: tensor([2.7819, 2.8836, 2.8860], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 800 | reward -117.08540000000005 | loss 55.685072313989195\n",
      "Q values: tensor([2.4659, 2.7436, 2.7452], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 83 | step 802 | reward -118.02822500000005 | loss 63.830468904222045\n",
      "Q values: tensor([2.0749, 2.3771, 2.4311], grad_fn=<SelectBackward>)\n",
      " 84%|████████▍ | 84/100 [1:02:28<10:29, 39.32s/it]\n",
      " Episode 84 | step 100 | reward -2.1951500000000004 | loss 0.9677682880493421\n",
      "Q values: tensor([1.7127, 1.6916, 1.6588], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 200 | reward -17.79385 | loss 6.6712716235452945\n",
      "Q values: tensor([1.5411, 1.6119, 1.5903], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 300 | reward -40.49099999999998 | loss 17.24050149229606\n",
      "Q values: tensor([1.8228, 1.8447, 1.8907], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 400 | reward -71.07864999999995 | loss 28.667050682542367\n",
      "Q values: tensor([1.7239, 1.8142, 1.9154], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 500 | reward -101.46387499999994 | loss 38.7517571780038\n",
      "Q values: tensor([0.9478, 1.1976, 1.3626], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 600 | reward -132.08832499999994 | loss 49.60692573875281\n",
      "Q values: tensor([0.7474, 1.0839, 1.3388], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 700 | reward -159.823175 | loss 56.02985606416701\n",
      "Q values: tensor([1.6749, 2.0101, 2.1492], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 800 | reward -186.29719999999992 | loss 63.466543938818404\n",
      "Q values: tensor([1.8461, 2.1508, 2.4766], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 84 | step 807 | reward -190.01584999999992 | loss 70.29763459306491\n",
      "Q values: tensor([1.5813, 1.9169, 2.2583], grad_fn=<SelectBackward>)\n",
      " 85%|████████▌ | 85/100 [1:03:09<09:56, 39.80s/it]\n",
      " Episode 85 | step 100 | reward -4.532450000000001 | loss 1.1049005896447852\n",
      "Q values: tensor([0.7271, 0.8321, 0.9297], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 85 | step 200 | reward -16.332424999999997 | loss 2.2063995371304372\n",
      "Q values: tensor([0.5822, 0.7203, 0.8295], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 85 | step 300 | reward -38.896400000000014 | loss 7.450034929208471\n",
      "Q values: tensor([0.4040, 0.5908, 0.7148], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 85 | step 400 | reward -68.161275 | loss 12.275808169371942\n",
      "Q values: tensor([0.6710, 0.8439, 0.9531], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 85 | step 500 | reward -82.84797500000002 | loss 14.391280200965184\n",
      "Q values: tensor([0.8866, 1.0408, 1.1092], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 85 | step 600 | reward -90.21790000000001 | loss 16.01005637485582\n",
      "Q values: tensor([0.9438, 1.1151, 1.1681], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 85 | step 700 | reward -104.02395000000003 | loss 19.716359037180382\n",
      "Q values: tensor([0.7320, 0.9587, 1.0054], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 85 | step 774 | reward -126.93190000000001 | loss 25.95024451658105\n",
      "Q values: tensor([-0.0950,  0.1193,  0.4219], grad_fn=<SelectBackward>)\n",
      " 86%|████████▌ | 86/100 [1:03:50<09:23, 40.23s/it]\n",
      " Episode 86 | step 100 | reward 4.707250000000001 | loss 1.7186411789336944\n",
      "Q values: tensor([0.9252, 0.9776, 1.0224], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 86 | step 200 | reward -1.6132499999999963 | loss 3.1408064896525048\n",
      "Q values: tensor([0.8819, 0.9740, 1.0464], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 86 | step 300 | reward -17.832074999999996 | loss 5.992118247080043\n",
      "Q values: tensor([1.1193, 1.2330, 1.3212], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 86 | step 400 | reward -34.96297500000001 | loss 9.407308744545162\n",
      "Q values: tensor([1.0231, 1.1856, 1.3424], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 86 | step 500 | reward -57.46712500000002 | loss 15.30154402963376\n",
      "Q values: tensor([1.4513, 1.6858, 1.8230], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 86 | step 600 | reward -84.42807500000004 | loss 21.81701142250654\n",
      "Q values: tensor([0.8284, 1.1348, 1.3151], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 86 | step 700 | reward -107.19587500000007 | loss 26.00163567175764\n",
      "Q values: tensor([1.6525, 1.7753, 1.9007], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 86 | step 780 | reward -116.35785000000007 | loss 31.243752419127077\n",
      "Q values: tensor([1.5707, 1.7235, 1.8194], grad_fn=<SelectBackward>)\n",
      " 87%|████████▋ | 87/100 [1:04:32<08:50, 40.82s/it]\n",
      " Episode 87 | step 100 | reward 2.5653750000000004 | loss 0.9968663969302725\n",
      "Q values: tensor([0.3237, 0.3780, 0.4345], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 87 | step 200 | reward 10.471425 | loss 3.8751664375686232\n",
      "Q values: tensor([0.1179, 0.1017, 0.1610], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 87 | step 300 | reward 29.122024999999997 | loss 12.525938595009983\n",
      "Q values: tensor([0.1189, 0.0730, 0.0841], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 87 | step 400 | reward 52.954499999999996 | loss 24.097024369189647\n",
      "Q values: tensor([0.5662, 0.5423, 0.3941], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 87 | step 500 | reward 75.09712499999999 | loss 34.90630195943745\n",
      "Q values: tensor([0.4107, 0.2999, 0.2262], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 87 | step 600 | reward 94.46767499999999 | loss 46.11516438407502\n",
      "Q values: tensor([1.2183, 1.0200, 0.9828], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 87 | step 700 | reward 113.63474999999995 | loss 54.7655752600805\n",
      "Q values: tensor([1.1554, 1.0441, 1.0058], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 87 | step 787 | reward 143.46072499999988 | loss 79.41269581739849\n",
      "Q values: tensor([2.0300, 1.9722, 1.8413], grad_fn=<SelectBackward>)\n",
      " 88%|████████▊ | 88/100 [1:05:07<07:47, 38.92s/it]\n",
      " Episode 88 | step 100 | reward -2.13365 | loss 0.3894637078642518\n",
      "Q values: tensor([0.9561, 0.9545, 0.9248], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 88 | step 200 | reward -9.651424999999996 | loss 1.9721081507891114\n",
      "Q values: tensor([1.0545, 1.0635, 1.0317], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 88 | step 300 | reward -20.050525000000007 | loss 4.711266952078361\n",
      "Q values: tensor([1.0922, 1.1125, 1.0953], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 88 | step 400 | reward -33.4463 | loss 8.171374208489354\n",
      "Q values: tensor([1.0763, 1.1180, 1.1239], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 88 | step 500 | reward -34.861274999999985 | loss 8.731506854040859\n",
      "Q values: tensor([1.2217, 1.2647, 1.2667], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 88 | step 600 | reward -32.379825 | loss 9.28117061519437\n",
      "Q values: tensor([1.8179, 1.8281, 1.7659], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 88 | step 700 | reward -35.92707500000003 | loss 10.66569243188755\n",
      "Q values: tensor([1.7704, 1.8010, 1.8110], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 88 | step 783 | reward -42.05342500000005 | loss 16.510120076613788\n",
      "Q values: tensor([1.5801, 1.7284, 1.7255], grad_fn=<SelectBackward>)\n",
      " 89%|████████▉ | 89/100 [1:05:47<07:12, 39.31s/it]\n",
      " Episode 89 | step 100 | reward 5.396125 | loss 0.5876720087685534\n",
      "Q values: tensor([0.7951, 0.7523, 0.7726], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 200 | reward 19.06107500000001 | loss 4.167980331759843\n",
      "Q values: tensor([1.0046, 0.9805, 0.9466], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 300 | reward 33.82405000000002 | loss 9.128217635818764\n",
      "Q values: tensor([1.1599, 1.1068, 1.0767], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 400 | reward 44.817375 | loss 12.078441802858542\n",
      "Q values: tensor([1.1462, 1.0381, 1.0665], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 500 | reward 68.090875 | loss 23.41881988218909\n",
      "Q values: tensor([1.8840, 1.7396, 1.6962], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 600 | reward 94.736725 | loss 37.85870031879556\n",
      "Q values: tensor([2.2151, 2.0407, 1.9730], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 700 | reward 115.40350000000004 | loss 49.087074439789085\n",
      "Q values: tensor([2.7443, 2.5243, 2.5433], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 800 | reward 114.49335 | loss 50.503243852253654\n",
      "Q values: tensor([2.9648, 2.9174, 2.8857], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 89 | step 807 | reward 114.32425 | loss 60.21882473835711\n",
      "Q values: tensor([2.9401, 2.8951, 2.8599], grad_fn=<SelectBackward>)\n",
      " 90%|█████████ | 90/100 [1:06:27<06:33, 39.40s/it]\n",
      " Episode 90 | step 100 | reward 5.010825 | loss 0.6255332879636626\n",
      "Q values: tensor([1.2893, 1.2316, 1.2001], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 90 | step 200 | reward 17.426799999999993 | loss 3.2270018580616693\n",
      "Q values: tensor([1.5953, 1.5486, 1.4789], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 90 | step 300 | reward 28.88409999999999 | loss 5.587366960480267\n",
      "Q values: tensor([1.8794, 1.8246, 1.7671], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 90 | step 400 | reward 35.93254999999997 | loss 6.2795578478769585\n",
      "Q values: tensor([1.9560, 1.9253, 1.8938], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 90 | step 500 | reward 53.60434999999999 | loss 10.91167385811957\n",
      "Q values: tensor([2.7556, 2.6540, 2.5734], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 90 | step 600 | reward 66.919275 | loss 13.992282743665896\n",
      "Q values: tensor([3.1802, 3.1001, 3.0577], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 90 | step 700 | reward 70.65312500000002 | loss 14.925788169303075\n",
      "Q values: tensor([3.5874, 3.5362, 3.5003], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 90 | step 751 | reward 69.25017500000003 | loss 31.533970929752\n",
      "Q values: tensor([3.7255, 3.6726, 3.6366], grad_fn=<SelectBackward>)\n",
      " 91%|█████████ | 91/100 [1:07:05<05:52, 39.19s/it]\n",
      " Episode 91 | step 100 | reward -5.390599999999999 | loss 1.0395636671802322\n",
      "Q values: tensor([1.4030, 1.3734, 1.3546], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 91 | step 200 | reward -5.245674999999996 | loss 2.129095988647066\n",
      "Q values: tensor([1.5248, 1.4953, 1.4799], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 91 | step 300 | reward 12.428125000000003 | loss 6.979601078249985\n",
      "Q values: tensor([2.0623, 1.9913, 1.8989], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 91 | step 400 | reward 21.857175000000005 | loss 8.690617876098557\n",
      "Q values: tensor([1.9694, 1.9551, 1.9114], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 91 | step 500 | reward 28.573525 | loss 9.286006787325121\n",
      "Q values: tensor([2.1635, 2.1337, 2.0640], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 91 | step 600 | reward 39.92822499999998 | loss 11.138737505394502\n",
      "Q values: tensor([2.8966, 2.8916, 2.8002], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 91 | step 700 | reward 56.61227499999999 | loss 15.343756852433117\n",
      "Q values: tensor([3.5462, 3.6161, 3.4681], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 91 | step 736 | reward 64.06762499999999 | loss 30.97405494949807\n",
      "Q values: tensor([3.9188, 4.0173, 3.8321], grad_fn=<SelectBackward>)\n",
      " 92%|█████████▏| 92/100 [1:07:43<05:10, 38.79s/it]\n",
      " Episode 92 | step 100 | reward -6.114325000000001 | loss 1.340875554122249\n",
      "Q values: tensor([1.4873, 1.5362, 1.5328], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 92 | step 200 | reward -11.825974999999998 | loss 2.2112442970083066\n",
      "Q values: tensor([1.6501, 1.7161, 1.6893], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 92 | step 300 | reward -15.9811 | loss 3.0059455565584017\n",
      "Q values: tensor([1.9238, 2.0104, 1.9874], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 92 | step 400 | reward -34.80687499999999 | loss 9.909207883669694\n",
      "Q values: tensor([1.9971, 2.1408, 2.1195], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 92 | step 500 | reward -51.07145000000001 | loss 15.816668613956097\n",
      "Q values: tensor([2.1107, 2.1624, 2.2072], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 92 | step 600 | reward -60.442599999999985 | loss 18.384753452927725\n",
      "Q values: tensor([2.5335, 2.5876, 2.6185], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 92 | step 700 | reward -76.80084999999994 | loss 23.251605103196198\n",
      "Q values: tensor([2.6887, 2.7155, 2.8981], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 92 | step 765 | reward -90.81629999999996 | loss 40.23966000470682\n",
      "Q values: tensor([2.4561, 2.5637, 2.6334], grad_fn=<SelectBackward>)\n",
      " 93%|█████████▎| 93/100 [1:08:23<04:33, 39.12s/it]\n",
      " Episode 93 | step 100 | reward 6.170325000000001 | loss 0.7166599320487563\n",
      "Q values: tensor([1.2103, 1.1845, 1.1744], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 93 | step 200 | reward 14.087674999999996 | loss 1.6388463192022442\n",
      "Q values: tensor([1.3329, 1.2790, 1.2837], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 93 | step 300 | reward 24.603424999999994 | loss 3.383164847856065\n",
      "Q values: tensor([1.5629, 1.4815, 1.4915], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 93 | step 400 | reward 39.852724999999985 | loss 7.193231046062252\n",
      "Q values: tensor([1.9028, 1.8169, 1.7946], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 93 | step 500 | reward 52.03674999999999 | loss 10.216610842581773\n",
      "Q values: tensor([2.3200, 2.1988, 2.2384], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 93 | step 600 | reward 58.34599999999995 | loss 11.066823605329674\n",
      "Q values: tensor([2.4307, 2.3407, 2.3586], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 93 | step 700 | reward 70.24047499999999 | loss 13.66350370294289\n",
      "Q values: tensor([3.2432, 3.1031, 3.1358], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 93 | step 776 | reward 72.520775 | loss 27.849292493738304\n",
      "Q values: tensor([3.5073, 3.4589, 3.4525], grad_fn=<SelectBackward>)\n",
      " 94%|█████████▍| 94/100 [1:09:04<03:58, 39.76s/it]\n",
      " Episode 94 | step 100 | reward 5.123099999999999 | loss 0.5022473218884898\n",
      "Q values: tensor([1.3089, 1.3018, 1.2628], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 94 | step 200 | reward 8.859824999999997 | loss 0.6390758072897711\n",
      "Q values: tensor([1.5116, 1.4749, 1.4554], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 94 | step 300 | reward 26.307624999999998 | loss 6.628646570336343\n",
      "Q values: tensor([1.9413, 1.8494, 1.8524], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 94 | step 400 | reward 46.72459999999999 | loss 13.526548225284671\n",
      "Q values: tensor([2.3382, 2.2454, 2.1904], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 94 | step 500 | reward 56.69954999999997 | loss 15.32269239028517\n",
      "Q values: tensor([2.4528, 2.3759, 2.3505], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 94 | step 600 | reward 71.63619999999996 | loss 18.647647717895666\n",
      "Q values: tensor([2.9331, 2.9380, 2.8133], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 94 | step 700 | reward 98.16357499999992 | loss 27.711059588102103\n",
      "Q values: tensor([3.8259, 3.8469, 3.6553], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 94 | step 760 | reward 116.47507499999995 | loss 53.60694775353102\n",
      "Q values: tensor([4.6805, 4.6576, 4.4986], grad_fn=<SelectBackward>)\n",
      " 95%|█████████▌| 95/100 [1:09:43<03:17, 39.45s/it]\n",
      " Episode 95 | step 100 | reward -0.08830000000000002 | loss 0.4392882988204434\n",
      "Q values: tensor([1.7564, 1.7266, 1.6678], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 95 | step 200 | reward -4.9976 | loss 1.492587727770598\n",
      "Q values: tensor([1.9138, 1.8884, 1.8578], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 95 | step 300 | reward -13.175749999999994 | loss 3.715278622864872\n",
      "Q values: tensor([2.1880, 2.1687, 2.1704], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 95 | step 400 | reward -14.3772 | loss 5.670403552517229\n",
      "Q values: tensor([2.7377, 2.6782, 2.6680], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 95 | step 500 | reward -6.320649999999994 | loss 6.725648164227124\n",
      "Q values: tensor([3.2194, 3.1720, 3.1189], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 95 | step 600 | reward 9.523900000000006 | loss 9.87808722733925\n",
      "Q values: tensor([3.9380, 3.9034, 3.7943], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 95 | step 700 | reward 25.93370000000001 | loss 12.939457437778941\n",
      "Q values: tensor([4.8313, 4.7221, 4.6499], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 95 | step 745 | reward 27.90675000000001 | loss 32.38675907220008\n",
      "Q values: tensor([4.2454, 4.2593, 4.1469], grad_fn=<SelectBackward>)\n",
      " 96%|█████████▌| 96/100 [1:10:20<02:34, 38.58s/it]\n",
      " Episode 96 | step 100 | reward 5.94035 | loss 0.7574332635863357\n",
      "Q values: tensor([1.7902, 1.7178, 1.7098], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 96 | step 200 | reward 16.702124999999995 | loss 2.7606245681898827\n",
      "Q values: tensor([2.4097, 2.2838, 2.2320], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 96 | step 300 | reward 18.890774999999994 | loss 3.3385099724191036\n",
      "Q values: tensor([2.4709, 2.4055, 2.3864], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 96 | step 400 | reward 18.095399999999984 | loss 4.1783659557486885\n",
      "Q values: tensor([3.0591, 2.9853, 2.9664], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 96 | step 500 | reward 12.901249999999981 | loss 6.622862630320981\n",
      "Q values: tensor([3.3191, 3.2704, 3.2975], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 96 | step 600 | reward 4.666199999999979 | loss 10.495255133316334\n",
      "Q values: tensor([3.2558, 3.2304, 3.2025], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 96 | step 700 | reward -1.1367500000000208 | loss 14.309263782716755\n",
      "Q values: tensor([4.0153, 3.9385, 3.8949], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 96 | step 751 | reward 0.974924999999979 | loss 34.63919419722794\n",
      "Q values: tensor([4.2282, 4.2123, 4.1521], grad_fn=<SelectBackward>)\n",
      " 97%|█████████▋| 97/100 [1:10:58<01:55, 38.49s/it]\n",
      " Episode 97 | step 100 | reward 3.2552750000000015 | loss 0.11348450414011779\n",
      "Q values: tensor([1.6931, 1.6661, 1.6580], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 97 | step 200 | reward 2.6678750000000035 | loss 0.5051640172146392\n",
      "Q values: tensor([1.8778, 1.8503, 1.8454], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 97 | step 300 | reward -4.856049999999996 | loss 2.5545822668087084\n",
      "Q values: tensor([1.9580, 1.9308, 1.9184], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 97 | step 400 | reward -0.12912499999999638 | loss 3.324574769397074\n",
      "Q values: tensor([2.5013, 2.4527, 2.4134], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 97 | step 500 | reward -0.5643749999999963 | loss 5.449724532041543\n",
      "Q values: tensor([2.4874, 2.5026, 2.5081], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 97 | step 600 | reward -15.25409999999999 | loss 12.91112371856103\n",
      "Q values: tensor([2.5467, 2.5356, 2.5993], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 97 | step 700 | reward -29.61499999999999 | loss 17.72393401419741\n",
      "Q values: tensor([2.7944, 2.6130, 2.7021], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 97 | step 758 | reward -35.739599999999996 | loss 33.84169810344325\n",
      "Q values: tensor([3.2098, 3.0823, 3.1909], grad_fn=<SelectBackward>)\n",
      " 98%|█████████▊| 98/100 [1:11:37<01:17, 38.70s/it]\n",
      " Episode 98 | step 100 | reward 5.05965 | loss 0.4784151824036371\n",
      "Q values: tensor([1.5619, 1.5246, 1.4888], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 98 | step 200 | reward 20.418050000000004 | loss 3.8715919492259445\n",
      "Q values: tensor([1.8384, 1.7923, 1.7291], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 98 | step 300 | reward 46.34182500000001 | loss 13.645510106556571\n",
      "Q values: tensor([2.1606, 2.1679, 2.0185], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 98 | step 400 | reward 82.04085000000005 | loss 31.802019054046355\n",
      "Q values: tensor([2.7479, 2.8282, 2.5668], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 98 | step 500 | reward 109.77047500000008 | loss 47.20407365251401\n",
      "Q values: tensor([3.0637, 3.2528, 2.9887], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 98 | step 600 | reward 147.5100750000001 | loss 77.51003956364536\n",
      "Q values: tensor([4.1641, 4.3193, 4.1042], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 98 | step 700 | reward 169.1271750000002 | loss 88.99047906182011\n",
      "Q values: tensor([4.2874, 4.5215, 4.2188], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 98 | step 737 | reward 180.1828000000001 | loss 115.56719959641111\n",
      "Q values: tensor([4.7609, 4.9542, 4.7381], grad_fn=<SelectBackward>)\n",
      " 99%|█████████▉| 99/100 [1:12:16<00:38, 38.70s/it]\n",
      " Episode 99 | step 100 | reward 7.206749999999999 | loss 0.8882120020082311\n",
      "Q values: tensor([1.8626, 1.7762, 1.8041], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 99 | step 200 | reward 19.492449999999998 | loss 3.4608518993061352\n",
      "Q values: tensor([2.1856, 2.0665, 2.0960], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 99 | step 300 | reward 24.247549999999993 | loss 4.087441998440795\n",
      "Q values: tensor([2.4473, 2.3737, 2.3729], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 99 | step 400 | reward 25.151774999999986 | loss 4.37771948829328\n",
      "Q values: tensor([2.7048, 2.6466, 2.6466], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 99 | step 500 | reward 17.72159999999999 | loss 6.730544912225071\n",
      "Q values: tensor([2.9637, 2.9552, 2.9450], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 99 | step 600 | reward 8.22425 | loss 10.531796228830274\n",
      "Q values: tensor([3.6335, 3.5760, 3.6336], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 99 | step 700 | reward -8.658425 | loss 18.207639766253862\n",
      "Q values: tensor([3.3990, 3.5537, 3.5848], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 99 | step 761 | reward -18.14772499999999 | loss 36.008953355589085\n",
      "Q values: tensor([3.4620, 3.6010, 3.5633], grad_fn=<SelectBackward>)\n",
      "100%|██████████| 100/100 [1:12:55<00:00, 43.75s/it]\n",
      "tensor([[ 1.0532e+00,  5.1828e-01, -7.6830e-01, -2.3441e+01,  9.2215e-01,\n",
      "          4.4840e-01,  1.3423e+00, -1.8194e+01,  6.6120e-01,  2.0572e-01,\n",
      "          1.2284e+00, -1.8369e+01, -5.7408e-01, -7.6768e-01,  7.7543e-01,\n",
      "         -1.1895e+00, -1.5678e+01, -1.5824e+01, -7.3383e-01,  1.3942e+00,\n",
      "          7.8305e-01,  2.7552e-01,  3.9566e-02, -1.3588e+01,  3.8042e-01,\n",
      "          3.6088e-02, -7.7383e-02, -1.3513e+01,  3.4569e+00,  2.6533e+00,\n",
      "          1.0654e+00, -1.3035e+00, -1.4365e+01, -1.4493e+01, -6.7449e-01,\n",
      "         -1.9588e-01,  4.3350e-01,  1.9114e-01,  3.0534e-02, -1.3624e+01,\n",
      "          2.5686e-01, -8.9047e-02, -5.2525e-02, -1.3710e+01,  3.4591e+00,\n",
      "          3.3489e+00,  5.6856e-01, -1.0563e+00, -1.4606e+01, -1.4653e+01,\n",
      "         -2.9134e-01, -4.6435e-01, -1.4548e-01, -1.3476e-01, -8.2840e-01,\n",
      "          1.8018e+00, -1.7534e-01, -1.1118e-01, -4.8242e-01,  7.5130e-01,\n",
      "          6.4373e-01,  2.6096e-01, -1.6928e+00, -6.4077e+00, -5.6491e-01,\n",
      "         -1.1099e+00,  4.4508e+00, -1.7958e+01, -1.0956e+01, -1.0887e+01,\n",
      "         -2.9272e+00,  2.0017e+00, -1.0885e+01, -1.0965e+01, -1.8151e+00,\n",
      "          1.8585e+00, -2.2542e-01,  1.1657e-01, -7.2489e+00,  9.9821e+00,\n",
      "          1.1733e+00,  8.6448e-01,  1.3730e+01, -5.3982e-01],\n",
      "        [ 9.6036e-01,  6.0762e-01, -7.0918e-01, -1.8036e+01,  8.9604e-01,\n",
      "          5.4990e-01,  9.8335e-01, -1.4199e+01,  7.3871e-01,  3.5197e-01,\n",
      "          9.8900e-01, -1.4167e+01, -5.7091e-01, -6.7882e-01,  7.7372e-01,\n",
      "         -6.3922e-01, -1.4132e+01, -1.4079e+01, -2.5054e-01,  1.2841e+00,\n",
      "          8.8322e-01,  5.4558e-01,  7.8602e-03, -1.0839e+01,  6.6388e-01,\n",
      "          1.5718e-01,  1.6685e-01, -1.0755e+01,  3.6052e+00,  3.2252e+00,\n",
      "          3.2104e-01, -7.0063e-01, -1.2882e+01, -1.2798e+01, -2.3890e-02,\n",
      "          2.9038e-01,  6.8527e-01,  4.0018e-01,  8.1588e-03, -1.0795e+01,\n",
      "          3.1622e-01,  8.5932e-02,  5.5839e-02, -1.0911e+01,  3.6631e+00,\n",
      "          3.8489e+00,  9.2146e-04, -5.0761e-01, -1.2803e+01, -1.2900e+01,\n",
      "          8.8549e-02,  1.8518e-01, -2.2171e-01, -3.5396e-01, -6.1220e-01,\n",
      "          1.2506e+00, -4.2750e-01, -3.3247e-01, -1.5904e-01,  2.9871e-01,\n",
      "          4.9266e-01,  3.2064e-01, -1.5537e+00, -5.0092e+00, -2.5631e-01,\n",
      "         -6.5329e-01,  4.0233e+00, -1.3648e+01, -1.0460e+01, -1.0409e+01,\n",
      "         -7.2257e-01,  1.6325e+00, -1.0493e+01, -1.0312e+01,  1.6270e-02,\n",
      "          1.4190e+00, -4.3237e-01, -2.1005e-01, -5.6549e+00,  6.7783e+00,\n",
      "          1.2569e+00,  8.9871e-01,  1.1548e+01, -3.7712e-01],\n",
      "        [ 8.4403e-01,  6.8233e-01, -6.2081e-01, -1.5209e+01,  9.2132e-01,\n",
      "          6.7981e-01,  9.4473e-01, -1.1990e+01,  6.0791e-01,  4.1962e-01,\n",
      "          8.3433e-01, -1.2191e+01, -3.6479e-01, -5.8406e-01,  7.1630e-01,\n",
      "         -2.9237e-01, -1.3062e+01, -1.2872e+01, -9.6931e-01,  2.1447e+00,\n",
      "          8.3790e-01,  7.1027e-01,  2.2767e-01, -9.0396e+00,  7.0799e-01,\n",
      "          2.8688e-01,  2.9028e-01, -9.2551e+00,  4.0199e+00,  3.2540e+00,\n",
      "          6.4591e-01, -2.5397e-01, -1.2191e+01, -1.2104e+01, -1.0830e+00,\n",
      "          8.7863e-01,  6.5317e-01,  4.9740e-01,  1.4733e-01, -9.1159e+00,\n",
      "          4.8813e-01,  1.8771e-01,  2.2023e-01, -9.2841e+00,  4.1898e+00,\n",
      "          4.1011e+00,  2.7029e-01, -2.6705e-01, -1.2292e+01, -1.2338e+01,\n",
      "         -7.5930e-01,  6.6603e-01, -3.0216e-01, -3.6559e-01, -4.5563e-01,\n",
      "          1.1028e+00, -4.9477e-01, -4.2896e-01, -2.0290e-01,  4.6281e-01,\n",
      "          6.2160e-01,  3.4349e-01, -1.1359e+00, -4.2042e+00, -1.0934e-01,\n",
      "         -4.6056e-01,  3.1143e+00, -1.0568e+01, -1.0215e+01, -1.0159e+01,\n",
      "         -2.8246e+00,  2.6367e+00, -1.0244e+01, -1.0167e+01, -2.0936e+00,\n",
      "          2.2895e+00, -4.0759e-01, -2.2884e-01, -4.5983e+00,  4.0121e+00,\n",
      "          1.4190e+00,  8.5867e-01,  9.4226e+00, -2.6038e-01]])\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "lr = 0.001\n",
    "window_length = None\n",
    "eps = 1.0\n",
    "eps_decay = utils.linear_decay(epochs=40_000, start=eps, end=0.1)\n",
    "\n",
    "sigpolicy = SigPolicy(env, 3, in_channels=4)\n",
    "sigpolicy.initialize_parameters(zero_bias=True)\n",
    "\n",
    "\n",
    "results = train(env, \n",
    "                sigpolicy, \n",
    "                episodes,\n",
    "                discount=0.97,\n",
    "                learning_rate=lr, \n",
    "                epsilon=eps,\n",
    "                epsilon_decay=eps_decay,\n",
    "                window_length=window_length, \n",
    "                printing=False)\n",
    "\n",
    "print(sigpolicy.linear.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff5511b68b0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(results[\"rewards\"])\n",
    "#plt.plot(results[\"actions\"][0])\n",
    "#plt.plot([reward if reward > -2000 else 0 for reward in results[\"rewards\"]])\n",
    "\n",
    "import numpy as np\n",
    "hist = list(results[\"history\"])\n",
    "plt.plot(np.array(hist)[:,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# write results \n",
    "with open('../results/error.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(results, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load results\n",
    "with open('../results/500_runs.pkl', 'rb') as f:\n",
    "    deserialized_results = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(deserialized_results[\"rewards\"])\n",
    "#plt.plot(results[\"losses\"])\n",
    "plt.plot([reward if reward > -2000 else 0 for reward in deserialized_results[\"rewards\"]])\n",
    "\n",
    "hist = list(deserialized_results[\"history\"])\n",
    "#plt.plot(np.array(hist)[:,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
