{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run base.ipynb\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from abides_gym_market_making_environment_v2 import *\n",
    "from policies_v1 import SigPolicy\n",
    "from train_v2 import train\n",
    "from test_v1_v2 import test\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register market making env for gym use \n",
    "from gym.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"market-making-v2\",\n",
    "    entry_point=SubGymMarketsMarketMakingEnv_v2,\n",
    ")\n",
    "\n",
    "def generate_env(seed=None):\n",
    "    \"\"\"\n",
    "    generates specific environment with the parameters defined and set the seed\n",
    "    \"\"\"\n",
    "    env = gym.make(\n",
    "            \"market-making-v2\",\n",
    "            background_config=\"rmsc04\",\n",
    "            mkt_close=\"10:45:00\",\n",
    "            timestep_duration=\"10s\",\n",
    "            order_fixed_size=10,\n",
    "            first_interval=\"00:10:00\", #00:13:00\n",
    "            observe_first_interval=False,\n",
    "            max_inventory=100,\n",
    "            mkt_order_alpha=0.2,\n",
    "            terminal_inventory_reward=2,# reward\n",
    "            inventory_reward_dampener=1., # 0.6,\n",
    "            damp_mode=\"asymmetric\",\n",
    "            debug_mode=False\n",
    "        )\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    \n",
    "    return env\n",
    "\n",
    "# create the environment\n",
    "env = generate_env(3) #5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a signature policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]\n",
      " Episode 0 | step 100 | reward -0.9569000000000001 | loss 0.07894234973591319\n",
      "Q values: tensor([ 0.0312, -0.0295, -0.0039, -0.0060], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 200 | reward -3.6793 | loss 0.8797528263123808\n",
      "Q values: tensor([-0.0108, -0.0493, -0.0482, -0.0379], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 300 | reward -4.397399999999999 | loss 0.9609459398162091\n",
      "Q values: tensor([ 0.0151, -0.0821, -0.0726, -0.0356], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 0 | step 391 | reward -4.899249999999999 | loss 1.026528331051308\n",
      "Q values: tensor([ 0.0086,  0.0311, -0.2404, -0.0045], grad_fn=<SelectBackward>)\n",
      "  0%|          | 1/250 [00:25<1:47:53, 26.00s/it]\n",
      " Episode 1 | step 400 | reward -0.027999999999999997 | loss 0.0010708907586831629\n",
      "Q values: tensor([ 0.0042,  0.0270, -0.0182, -0.0042], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 500 | reward -1.1355 | loss 0.35640699244041096\n",
      "Q values: tensor([ 0.0540,  0.0847, -0.0937, -0.0346], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 600 | reward -1.4674999999999985 | loss 0.4444260191478415\n",
      "Q values: tensor([-0.0227, -0.0490,  0.0185, -0.0430], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 700 | reward -2.414399999999998 | loss 0.534590332683492\n",
      "Q values: tensor([0.0152, 0.0023, 0.0230, 0.0018], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 1 | step 782 | reward -0.9774999999999987 | loss 1.8922898940150403\n",
      "Q values: tensor([0.0814, 0.0870, 0.0353, 0.0678], grad_fn=<SelectBackward>)\n",
      "  1%|          | 2/250 [00:48<1:39:51, 24.16s/it]\n",
      " Episode 2 | step 800 | reward -0.011000000000000003 | loss 0.0018766145262887335\n",
      "Q values: tensor([ 0.0171,  0.0277, -0.0007,  0.0194], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 900 | reward -0.7075000000000001 | loss 0.2530979645601611\n",
      "Q values: tensor([ 0.0325,  0.0342, -0.0255,  0.0100], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 1000 | reward -1.9858499999999994 | loss 0.5375390448528012\n",
      "Q values: tensor([ 0.0742,  0.0745, -0.0378,  0.0078], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 1100 | reward -2.5907999999999998 | loss 0.6047833813424336\n",
      "Q values: tensor([ 0.0119, -0.0232,  0.0367,  0.0262], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 2 | step 1173 | reward -2.7860000000000023 | loss 1.0178808386990577\n",
      "Q values: tensor([0.0435, 0.0691, 0.0843, 0.0795], grad_fn=<SelectBackward>)\n",
      "  1%|          | 3/250 [01:12<1:38:49, 24.01s/it]\n",
      " Episode 3 | step 1200 | reward -0.009500000000000001 | loss 0.0029048657595218685\n",
      "Q values: tensor([0.0124, 0.0192, 0.0292, 0.0403], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 1300 | reward -0.9530000000000002 | loss 0.02882363362592344\n",
      "Q values: tensor([0.0485, 0.0207, 0.0337, 0.0457], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 1400 | reward -1.9095000000000006 | loss 0.10865505947986043\n",
      "Q values: tensor([0.0481, 0.0004, 0.0518, 0.0432], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 1500 | reward -3.621000000000002 | loss 0.19417556306667294\n",
      "Q values: tensor([0.0339, 0.0067, 0.0133, 0.0346], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 3 | step 1564 | reward -3.657800000000003 | loss 0.22997801091226522\n",
      "Q values: tensor([0.0237, 0.0038, 0.0395, 0.0462], grad_fn=<SelectBackward>)\n",
      "  2%|▏         | 4/250 [01:34<1:35:22, 23.26s/it]\n",
      " Episode 4 | step 1600 | reward -0.23780000000000004 | loss 0.005861129955230915\n",
      "Q values: tensor([0.0311, 0.0677, 0.0335, 0.0535], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 1700 | reward -0.4996000000000002 | loss 0.020614924964069914\n",
      "Q values: tensor([0.0234, 0.0196, 0.0471, 0.0330], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 1800 | reward -0.7806000000000001 | loss 0.04685458955087575\n",
      "Q values: tensor([0.0636, 0.0320, 0.0104, 0.0324], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 1900 | reward -2.0856000000000003 | loss 0.24687349558769567\n",
      "Q values: tensor([ 0.0429,  0.0178, -0.0151,  0.0585], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 4 | step 1955 | reward -2.313850000000001 | loss 0.33846889119952717\n",
      "Q values: tensor([ 0.0520, -0.0004, -0.1406,  0.0874], grad_fn=<SelectBackward>)\n",
      "  2%|▏         | 5/250 [01:58<1:34:54, 23.24s/it]\n",
      " Episode 5 | step 2000 | reward -0.201 | loss 0.04629646576957358\n",
      "Q values: tensor([ 0.0493,  0.0975, -0.0268,  0.0357], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 2100 | reward -1.4865000000000002 | loss 0.1472520278527707\n",
      "Q values: tensor([ 0.0790,  0.0944, -0.0047,  0.0532], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 2200 | reward -2.42535 | loss 0.23217897775893678\n",
      "Q values: tensor([0.0354, 0.0174, 0.0717, 0.0295], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 2300 | reward -3.0227000000000013 | loss 0.29329617307185407\n",
      "Q values: tensor([0.1145, 0.1365, 0.1702, 0.1605], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 5 | step 2346 | reward -1.5377000000000005 | loss 1.246779121543343\n",
      "Q values: tensor([0.1640, 0.1709, 0.1693, 0.1793], grad_fn=<SelectBackward>)\n",
      "  2%|▏         | 6/250 [02:20<1:33:26, 22.98s/it]\n",
      " Episode 6 | step 2400 | reward -0.5485000000000001 | loss 0.06220053159946515\n",
      "Q values: tensor([0.0348, 0.0379, 0.0629, 0.0561], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 2500 | reward -0.8843 | loss 0.09789940189638457\n",
      "Q values: tensor([0.0524, 0.0282, 0.0729, 0.0460], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 2600 | reward -3.56815 | loss 0.2868033685447149\n",
      "Q values: tensor([0.1058, 0.0721, 0.0127, 0.0899], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 2700 | reward -4.571900000000001 | loss 0.3568167789510033\n",
      "Q values: tensor([0.1087, 0.0755, 0.0535, 0.0967], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 6 | step 2737 | reward -4.673000000000002 | loss 0.3751844358942724\n",
      "Q values: tensor([0.1210, 0.0940, 0.0708, 0.1006], grad_fn=<SelectBackward>)\n",
      "  3%|▎         | 7/250 [02:40<1:28:49, 21.93s/it]\n",
      " Episode 7 | step 2800 | reward -0.14450000000000002 | loss 0.016452985027006095\n",
      "Q values: tensor([0.0325, 0.0431, 0.0547, 0.0565], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 2900 | reward -0.6636000000000002 | loss 0.02987806409653477\n",
      "Q values: tensor([0.0552, 0.0847, 0.0300, 0.0694], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 3000 | reward -1.3332500000000005 | loss 0.06755760490215354\n",
      "Q values: tensor([0.0674, 0.0429, 0.0706, 0.0753], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 3100 | reward -1.9231500000000006 | loss 0.15845813429827468\n",
      "Q values: tensor([0.1920, 0.1438, 0.0156, 0.1774], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 7 | step 3128 | reward -1.5377500000000004 | loss 0.2193515556136307\n",
      "Q values: tensor([0.1379, 0.1177, 0.1426, 0.1170], grad_fn=<SelectBackward>)\n",
      "  3%|▎         | 8/250 [03:03<1:30:07, 22.35s/it]\n",
      " Episode 8 | step 3200 | reward -0.21700000000000003 | loss 0.01165261352510083\n",
      "Q values: tensor([0.0546, 0.0749, 0.0702, 0.0660], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 3300 | reward -1.4534500000000008 | loss 0.05291686110001684\n",
      "Q values: tensor([0.0490, 0.0518, 0.0811, 0.0775], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 3400 | reward -2.0150000000000015 | loss 0.09919905158117273\n",
      "Q values: tensor([0.0790, 0.0703, 0.0722, 0.1026], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 3500 | reward -2.572500000000002 | loss 0.17806342333597927\n",
      "Q values: tensor([0.1557, 0.1349, 0.0173, 0.1537], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 8 | step 3519 | reward -2.879900000000002 | loss 0.1996852767849633\n",
      "Q values: tensor([0.1506, 0.1346, 0.0674, 0.1554], grad_fn=<SelectBackward>)\n",
      "  4%|▎         | 9/250 [03:26<1:30:32, 22.54s/it]\n",
      " Episode 9 | step 3600 | reward -0.28115 | loss 0.017518631503113724\n",
      "Q values: tensor([0.0848, 0.1071, 0.0714, 0.0738], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 3700 | reward -0.3943999999999999 | loss 0.061224485386484506\n",
      "Q values: tensor([0.0976, 0.0718, 0.0975, 0.0626], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 3800 | reward -1.4732999999999996 | loss 0.13566056424187733\n",
      "Q values: tensor([0.1229, 0.1233, 0.0921, 0.1010], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 3900 | reward -2.536649999999999 | loss 0.2114190348616184\n",
      "Q values: tensor([0.1202, 0.0879, 0.1243, 0.1213], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 9 | step 3910 | reward -0.8077499999999993 | loss 1.3143122096773516\n",
      "Q values: tensor([0.1270, 0.0902, 0.1252, 0.1270], grad_fn=<SelectBackward>)\n",
      "  4%|▍         | 10/250 [03:48<1:29:16, 22.32s/it]\n",
      " Episode 10 | step 4000 | reward -0.26930000000000004 | loss 0.0353668060401906\n",
      "Q values: tensor([0.0929, 0.1389, 0.0563, 0.0843], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 4100 | reward -1.2315000000000003 | loss 0.09527721013043622\n",
      "Q values: tensor([0.1196, 0.1439, 0.1046, 0.1068], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 4200 | reward -2.3790000000000004 | loss 0.14576129078560496\n",
      "Q values: tensor([0.1695, 0.1783, 0.1528, 0.1298], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 4300 | reward -2.805200000000001 | loss 0.2712002339334252\n",
      "Q values: tensor([0.3891, 0.4295, 0.3471, 0.3258], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 10 | step 4301 | reward -2.795400000000001 | loss 0.3179534318418267\n",
      "Q values: tensor([0.3631, 0.3956, 0.3490, 0.3156], grad_fn=<SelectBackward>)\n",
      "  4%|▍         | 11/250 [04:11<1:30:28, 22.71s/it]\n",
      " Episode 11 | step 4400 | reward -0.55455 | loss 0.0243454321189244\n",
      "Q values: tensor([0.1005, 0.1226, 0.0801, 0.1019], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 4500 | reward -1.6846000000000008 | loss 0.06915231958026205\n",
      "Q values: tensor([0.1399, 0.1668, 0.1239, 0.1267], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 4600 | reward -2.249600000000001 | loss 0.11513547914869485\n",
      "Q values: tensor([0.1637, 0.1869, 0.1471, 0.1930], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 11 | step 4692 | reward -2.564600000000002 | loss 0.7374427699179268\n",
      "Q values: tensor([0.2105, 0.1926, 0.2052, 0.2453], grad_fn=<SelectBackward>)\n",
      "  5%|▍         | 12/250 [04:35<1:31:22, 23.04s/it]\n",
      " Episode 12 | step 4700 | reward -0.002 | loss 0.0006106748710981158\n",
      "Q values: tensor([0.0595, 0.0719, 0.0874, 0.0859], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 4800 | reward -0.3358499999999999 | loss 0.015013945715753874\n",
      "Q values: tensor([0.0853, 0.0840, 0.1083, 0.0868], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 4900 | reward -1.5327999999999995 | loss 0.10495958644327708\n",
      "Q values: tensor([0.1179, 0.1178, 0.1323, 0.1139], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 5000 | reward -3.432749999999999 | loss 0.2565317234047999\n",
      "Q values: tensor([0.1362, 0.1339, 0.1761, 0.1381], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 12 | step 5083 | reward -3.1850999999999994 | loss 0.3036652078549942\n",
      "Q values: tensor([0.2112, 0.2328, 0.2059, 0.2243], grad_fn=<SelectBackward>)\n",
      "  5%|▌         | 13/250 [05:00<1:32:49, 23.50s/it]\n",
      " Episode 13 | step 5100 | reward -0.039 | loss 0.00225038670122224\n",
      "Q values: tensor([0.0711, 0.0774, 0.0930, 0.0956], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 5200 | reward -0.3899 | loss 0.014772370352352043\n",
      "Q values: tensor([0.0994, 0.0941, 0.1066, 0.1039], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 5300 | reward -1.0849000000000002 | loss 0.051056684819771464\n",
      "Q values: tensor([0.1372, 0.1324, 0.0942, 0.1193], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 5400 | reward -1.8109000000000002 | loss 0.0955469334026823\n",
      "Q values: tensor([0.2317, 0.2131, 0.1801, 0.2358], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 13 | step 5474 | reward -2.5009 | loss 0.139421228596094\n",
      "Q values: tensor([0.2438, 0.2476, 0.2237, 0.2548], grad_fn=<SelectBackward>)\n",
      "  6%|▌         | 14/250 [05:24<1:33:28, 23.77s/it]\n",
      " Episode 14 | step 5500 | reward -0.0625 | loss 0.003018933877380725\n",
      "Q values: tensor([0.0838, 0.0994, 0.0827, 0.0965], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 5600 | reward -0.8371000000000002 | loss 0.023362467580044344\n",
      "Q values: tensor([0.1292, 0.1400, 0.1216, 0.1251], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 5700 | reward -1.1531999999999996 | loss 0.044298279298714105\n",
      "Q values: tensor([0.1738, 0.1813, 0.1984, 0.1837], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 5800 | reward -2.0635 | loss 0.0993392744665933\n",
      "Q values: tensor([0.2350, 0.2373, 0.2616, 0.2439], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 14 | step 5865 | reward -2.778800000000001 | loss 0.16839237506582916\n",
      "Q values: tensor([0.3237, 0.3121, 0.2880, 0.3168], grad_fn=<SelectBackward>)\n",
      "  6%|▌         | 15/250 [05:48<1:32:46, 23.69s/it]\n",
      " Episode 15 | step 5900 | reward -0.1395 | loss 0.004155266572219984\n",
      "Q values: tensor([0.1118, 0.1278, 0.0874, 0.1141], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 6000 | reward -1.2844 | loss 0.03414020781306931\n",
      "Q values: tensor([0.1513, 0.1546, 0.1365, 0.1389], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 6100 | reward -3.3563 | loss 0.25652861102824304\n",
      "Q values: tensor([0.1693, 0.1379, 0.1735, 0.1664], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 6200 | reward -3.6072 | loss 0.2884640043100917\n",
      "Q values: tensor([0.3334, 0.2774, 0.3016, 0.3126], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 15 | step 6256 | reward -4.308800000000002 | loss 0.4236837439911825\n",
      "Q values: tensor([0.4388, 0.4367, 0.4612, 0.4468], grad_fn=<SelectBackward>)\n",
      "  6%|▋         | 16/250 [06:11<1:31:29, 23.46s/it]\n",
      " Episode 16 | step 6300 | reward -0.28500000000000003 | loss 0.01002271883072181\n",
      "Q values: tensor([0.1124, 0.1407, 0.0830, 0.1249], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 6400 | reward -1.0029000000000001 | loss 0.03963216045435125\n",
      "Q values: tensor([0.1437, 0.1491, 0.1283, 0.1462], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 6500 | reward -1.4002 | loss 0.05442063403389863\n",
      "Q values: tensor([0.1781, 0.1638, 0.1858, 0.1663], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 6600 | reward -2.1942999999999993 | loss 0.09410114974901118\n",
      "Q values: tensor([0.3269, 0.3442, 0.3584, 0.3552], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 16 | step 6647 | reward -1.0346999999999997 | loss 0.6521598791375456\n",
      "Q values: tensor([0.3300, 0.3408, 0.3594, 0.3525], grad_fn=<SelectBackward>)\n",
      "  7%|▋         | 17/250 [06:34<1:30:44, 23.37s/it]\n",
      " Episode 17 | step 6700 | reward -0.025650000000000003 | loss 0.00401858795451307\n",
      "Q values: tensor([0.1060, 0.1025, 0.1024, 0.1054], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 6800 | reward -0.6927000000000002 | loss 0.026785515120600373\n",
      "Q values: tensor([0.1483, 0.1420, 0.1431, 0.1517], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 6900 | reward -1.55505 | loss 0.06272580072367942\n",
      "Q values: tensor([0.2110, 0.1757, 0.2180, 0.2197], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 7000 | reward -2.7536000000000005 | loss 0.10641429274729189\n",
      "Q values: tensor([0.4196, 0.4183, 0.4294, 0.4162], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 17 | step 7038 | reward -2.895800000000001 | loss 0.1454103432295956\n",
      "Q values: tensor([0.4296, 0.4305, 0.4499, 0.4317], grad_fn=<SelectBackward>)\n",
      "  7%|▋         | 18/250 [06:56<1:29:08, 23.05s/it]\n",
      " Episode 18 | step 7100 | reward -0.3115 | loss 0.011025935424359284\n",
      "Q values: tensor([0.0996, 0.0879, 0.1159, 0.1127], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 7200 | reward -1.0180999999999998 | loss 0.034217830621886686\n",
      "Q values: tensor([0.1287, 0.1097, 0.1431, 0.1458], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 7300 | reward -1.5927499999999997 | loss 0.052333553234135716\n",
      "Q values: tensor([0.1887, 0.1683, 0.1907, 0.1631], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 7400 | reward -2.554100000000001 | loss 0.07059344659545044\n",
      "Q values: tensor([0.2390, 0.2201, 0.2341, 0.2190], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 18 | step 7429 | reward -2.192850000000001 | loss 0.14793011833301986\n",
      "Q values: tensor([0.2580, 0.2226, 0.2418, 0.2456], grad_fn=<SelectBackward>)\n",
      "  8%|▊         | 19/250 [07:21<1:31:28, 23.76s/it]\n",
      " Episode 19 | step 7500 | reward -0.18950000000000003 | loss 0.006439991180137911\n",
      "Q values: tensor([0.1256, 0.1522, 0.0989, 0.1440], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 7600 | reward -1.5829999999999995 | loss 0.041403883813860703\n",
      "Q values: tensor([0.1470, 0.1476, 0.1475, 0.1537], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 7700 | reward -2.423849999999999 | loss 0.08286227979009472\n",
      "Q values: tensor([0.1988, 0.1779, 0.2065, 0.2000], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 7800 | reward -3.128500000000001 | loss 0.10755678052840131\n",
      "Q values: tensor([0.3326, 0.3308, 0.3455, 0.3370], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 19 | step 7820 | reward -3.066350000000001 | loss 0.12817819931445804\n",
      "Q values: tensor([0.3618, 0.3618, 0.3731, 0.3634], grad_fn=<SelectBackward>)\n",
      "  8%|▊         | 20/250 [07:45<1:30:43, 23.67s/it]\n",
      " Episode 20 | step 7900 | reward -0.3473 | loss 0.009129354110234633\n",
      "Q values: tensor([0.1090, 0.1173, 0.1326, 0.1303], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 8000 | reward -0.6160000000000002 | loss 0.025899426690611183\n",
      "Q values: tensor([0.1483, 0.1388, 0.1729, 0.1470], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 8100 | reward -1.5811000000000002 | loss 0.061039406091343285\n",
      "Q values: tensor([0.2015, 0.1894, 0.2165, 0.2008], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 8200 | reward -2.9995499999999993 | loss 0.10741234207995776\n",
      "Q values: tensor([0.3511, 0.3588, 0.3329, 0.3481], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 20 | step 8211 | reward -3.132499999999999 | loss 0.16956074316164393\n",
      "Q values: tensor([0.3486, 0.3662, 0.3453, 0.3530], grad_fn=<SelectBackward>)\n",
      "  8%|▊         | 21/250 [08:07<1:28:55, 23.30s/it]\n",
      " Episode 21 | step 8300 | reward -0.10950000000000003 | loss 0.007357863720248853\n",
      "Q values: tensor([0.1110, 0.1105, 0.1296, 0.1174], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 8400 | reward -1.05645 | loss 0.03684699301273123\n",
      "Q values: tensor([0.1704, 0.1791, 0.1644, 0.1700], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 8500 | reward -1.7140499999999999 | loss 0.05529106163347022\n",
      "Q values: tensor([0.1966, 0.1992, 0.2164, 0.2057], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 8600 | reward -2.4764999999999997 | loss 0.08286847606050243\n",
      "Q values: tensor([0.4239, 0.4645, 0.4359, 0.4407], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 21 | step 8602 | reward -2.5322999999999998 | loss 0.2060146153786259\n",
      "Q values: tensor([0.4477, 0.4960, 0.4550, 0.4619], grad_fn=<SelectBackward>)\n",
      "  9%|▉         | 22/250 [08:31<1:28:59, 23.42s/it]\n",
      " Episode 22 | step 8700 | reward -0.3643 | loss 0.010323799252078936\n",
      "Q values: tensor([0.1406, 0.1630, 0.1319, 0.1514], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 8800 | reward -0.9178500000000003 | loss 0.030242498829996856\n",
      "Q values: tensor([0.1864, 0.2105, 0.2043, 0.2107], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 8900 | reward -1.6045500000000001 | loss 0.05642753152647162\n",
      "Q values: tensor([0.2276, 0.2338, 0.2582, 0.2492], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 22 | step 8993 | reward -2.25685 | loss 0.14326364437550343\n",
      "Q values: tensor([0.3915, 0.3924, 0.3650, 0.4120], grad_fn=<SelectBackward>)\n",
      "  9%|▉         | 23/250 [08:54<1:28:37, 23.42s/it]\n",
      " Episode 23 | step 9000 | reward -0.018 | loss 0.0010669351904167845\n",
      "Q values: tensor([0.0962, 0.1142, 0.1008, 0.1171], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 9100 | reward -0.5035000000000001 | loss 0.011914184735126121\n",
      "Q values: tensor([0.1456, 0.1296, 0.1328, 0.1330], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 9200 | reward -1.1793 | loss 0.0404245904394574\n",
      "Q values: tensor([0.1663, 0.1309, 0.1646, 0.1521], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 9300 | reward -1.5439 | loss 0.06583317123145199\n",
      "Q values: tensor([0.1801, 0.1744, 0.2135, 0.2202], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 23 | step 9384 | reward -0.4425999999999999 | loss 0.7152472685403188\n",
      "Q values: tensor([0.2108, 0.2139, 0.2429, 0.2336], grad_fn=<SelectBackward>)\n",
      " 10%|▉         | 24/250 [09:19<1:29:22, 23.73s/it]\n",
      " Episode 24 | step 9400 | reward -0.11399999999999999 | loss 0.0008596478183842404\n",
      "Q values: tensor([0.1157, 0.1458, 0.1198, 0.1396], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 9500 | reward -0.7061999999999999 | loss 0.020085844529673574\n",
      "Q values: tensor([0.1580, 0.1589, 0.1520, 0.1540], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 9600 | reward -1.5705999999999993 | loss 0.04825959119951309\n",
      "Q values: tensor([0.1853, 0.1744, 0.1680, 0.1757], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 9700 | reward -2.190299999999999 | loss 0.06947239466329916\n",
      "Q values: tensor([0.1894, 0.1971, 0.2263, 0.2198], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 24 | step 9775 | reward -2.7716999999999983 | loss 0.15303371726257622\n",
      "Q values: tensor([0.2567, 0.3255, 0.2976, 0.3276], grad_fn=<SelectBackward>)\n",
      " 10%|█         | 25/250 [09:43<1:28:59, 23.73s/it]\n",
      " Episode 25 | step 9800 | reward -0.0631 | loss 0.003589763994110484\n",
      "Q values: tensor([0.1150, 0.1371, 0.1127, 0.1248], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 9900 | reward -0.82795 | loss 0.03404580293299908\n",
      "Q values: tensor([0.1542, 0.1680, 0.1616, 0.1626], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 10000 | reward -1.2440499999999994 | loss 0.055477837451601175\n",
      "Q values: tensor([0.2321, 0.2520, 0.2738, 0.2546], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 10100 | reward -3.2577999999999987 | loss 0.36659846094596205\n",
      "Q values: tensor([0.3922, 0.3576, 0.3229, 0.3523], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 25 | step 10166 | reward -3.2428999999999992 | loss 0.4623208344510026\n",
      "Q values: tensor([0.3766, 0.3675, 0.4022, 0.4179], grad_fn=<SelectBackward>)\n",
      " 10%|█         | 26/250 [10:07<1:29:12, 23.89s/it]\n",
      " Episode 26 | step 10200 | reward -0.085 | loss 0.002619686955466327\n",
      "Q values: tensor([0.1306, 0.1313, 0.1336, 0.1367], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 10300 | reward -2.0547499999999994 | loss 0.08120954013628934\n",
      "Q values: tensor([0.1752, 0.1788, 0.1776, 0.1818], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 10400 | reward -2.5316999999999994 | loss 0.10210758781607021\n",
      "Q values: tensor([0.2422, 0.2251, 0.2386, 0.2171], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 10500 | reward -2.6794499999999988 | loss 0.1320820089362713\n",
      "Q values: tensor([0.3133, 0.2825, 0.3216, 0.2883], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 26 | step 10557 | reward -3.6931999999999983 | loss 0.29112500725410095\n",
      "Q values: tensor([0.4923, 0.4846, 0.4770, 0.4879], grad_fn=<SelectBackward>)\n",
      " 11%|█         | 27/250 [10:33<1:31:47, 24.70s/it]\n",
      " Episode 27 | step 10600 | reward -0.0325 | loss 0.0023187436105572834\n",
      "Q values: tensor([0.1346, 0.1311, 0.1384, 0.1388], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 10700 | reward -0.8031999999999999 | loss 0.01824357048478742\n",
      "Q values: tensor([0.1905, 0.1876, 0.1555, 0.1854], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 10800 | reward -1.3908999999999996 | loss 0.03654428468723758\n",
      "Q values: tensor([0.2225, 0.1893, 0.2297, 0.1973], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 10900 | reward -2.1013999999999995 | loss 0.07181905723778048\n",
      "Q values: tensor([0.2827, 0.2575, 0.2842, 0.2674], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 27 | step 10948 | reward -3.206199999999998 | loss 0.1957224256054212\n",
      "Q values: tensor([0.3812, 0.3566, 0.3989, 0.4026], grad_fn=<SelectBackward>)\n",
      " 11%|█         | 28/250 [10:56<1:29:18, 24.14s/it]\n",
      " Episode 28 | step 11000 | reward -0.21340000000000003 | loss 0.008307454830118632\n",
      "Q values: tensor([0.1562, 0.1730, 0.1420, 0.1741], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 11100 | reward -1.2013500000000001 | loss 0.050719000520301094\n",
      "Q values: tensor([0.1838, 0.1593, 0.1944, 0.1783], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 11200 | reward -1.68685 | loss 0.07267708054480604\n",
      "Q values: tensor([0.2219, 0.1933, 0.2317, 0.2119], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 11300 | reward -2.6041 | loss 0.11371718393929964\n",
      "Q values: tensor([0.3018, 0.3001, 0.3163, 0.3138], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 28 | step 11339 | reward -2.8281000000000005 | loss 0.20486340959520444\n",
      "Q values: tensor([0.3740, 0.3670, 0.3780, 0.4017], grad_fn=<SelectBackward>)\n",
      " 12%|█▏        | 29/250 [11:20<1:28:07, 23.93s/it]\n",
      " Episode 29 | step 11400 | reward -0.36610000000000004 | loss 0.009767565981615434\n",
      "Q values: tensor([0.1595, 0.1851, 0.1555, 0.1755], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 11500 | reward -0.7972000000000001 | loss 0.020131286142817117\n",
      "Q values: tensor([0.1972, 0.1900, 0.1931, 0.1897], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 11600 | reward -1.0039 | loss 0.029445292178178595\n",
      "Q values: tensor([0.2335, 0.2168, 0.2390, 0.2216], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 11700 | reward -2.0571 | loss 0.08825911950052856\n",
      "Q values: tensor([0.3061, 0.3076, 0.3015, 0.2896], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 29 | step 11730 | reward -2.156 | loss 0.1316188522286349\n",
      "Q values: tensor([0.3421, 0.3467, 0.3395, 0.3114], grad_fn=<SelectBackward>)\n",
      " 12%|█▏        | 30/250 [11:43<1:27:28, 23.86s/it]\n",
      " Episode 30 | step 11800 | reward -0.3630000000000001 | loss 0.010712556007080654\n",
      "Q values: tensor([0.1689, 0.1918, 0.1618, 0.1804], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 11900 | reward -1.90235 | loss 0.06567655525850613\n",
      "Q values: tensor([0.2040, 0.1979, 0.1987, 0.1967], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 12000 | reward -2.206200000000001 | loss 0.07780767925543375\n",
      "Q values: tensor([0.2414, 0.2678, 0.2589, 0.2647], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 12100 | reward -2.693400000000001 | loss 0.09761442489625483\n",
      "Q values: tensor([0.3210, 0.3400, 0.3222, 0.3393], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 30 | step 12121 | reward -1.541000000000001 | loss 0.43688981663551874\n",
      "Q values: tensor([0.3365, 0.3403, 0.3096, 0.3726], grad_fn=<SelectBackward>)\n",
      " 12%|█▏        | 31/250 [12:06<1:25:51, 23.52s/it]\n",
      " Episode 31 | step 12200 | reward -0.26000000000000006 | loss 0.003520756291608751\n",
      "Q values: tensor([0.1724, 0.1694, 0.1759, 0.1722], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 12300 | reward -0.7840000000000003 | loss 0.01862912838664499\n",
      "Q values: tensor([0.2179, 0.1985, 0.2049, 0.2003], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 12400 | reward -1.6956999999999995 | loss 0.06486635076443903\n",
      "Q values: tensor([0.3087, 0.2705, 0.2856, 0.2612], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 12500 | reward -3.530000000000001 | loss 0.259868518668262\n",
      "Q values: tensor([0.3967, 0.4122, 0.3613, 0.3834], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 31 | step 12512 | reward -3.6271000000000013 | loss 0.3584125023232251\n",
      "Q values: tensor([0.4426, 0.4532, 0.4221, 0.4043], grad_fn=<SelectBackward>)\n",
      " 13%|█▎        | 32/250 [12:29<1:25:13, 23.46s/it]\n",
      " Episode 32 | step 12600 | reward -0.3500000000000001 | loss 0.00997552605495411\n",
      "Q values: tensor([0.1796, 0.1974, 0.1859, 0.2000], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 12700 | reward -1.2416 | loss 0.029483341096883997\n",
      "Q values: tensor([0.2369, 0.2381, 0.2313, 0.2353], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 12800 | reward -1.5836499999999996 | loss 0.04129409268865514\n",
      "Q values: tensor([0.2978, 0.2903, 0.2943, 0.2821], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 12900 | reward -2.53395 | loss 0.12450337274803358\n",
      "Q values: tensor([0.4059, 0.3843, 0.4119, 0.4240], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 32 | step 12903 | reward -0.8041500000000001 | loss 0.9507976009566437\n",
      "Q values: tensor([0.4035, 0.3836, 0.4123, 0.4153], grad_fn=<SelectBackward>)\n",
      " 13%|█▎        | 33/250 [12:54<1:26:08, 23.82s/it]\n",
      " Episode 33 | step 13000 | reward -0.16835000000000003 | loss 0.005419790171629302\n",
      "Q values: tensor([0.1789, 0.1695, 0.1937, 0.1724], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 13100 | reward -1.6509500000000004 | loss 0.05766069989482547\n",
      "Q values: tensor([0.2374, 0.2291, 0.2372, 0.2412], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 13200 | reward -2.64385 | loss 0.0958021998951617\n",
      "Q values: tensor([0.3893, 0.3973, 0.3807, 0.4059], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 33 | step 13294 | reward -3.0079499999999992 | loss 0.660546465572879\n",
      "Q values: tensor([0.4638, 0.4767, 0.4703, 0.4694], grad_fn=<SelectBackward>)\n",
      " 14%|█▎        | 34/250 [13:19<1:26:50, 24.12s/it]\n",
      " Episode 34 | step 13300 | reward -0.035500000000000004 | loss 0.0002116371521592164\n",
      "Q values: tensor([0.1404, 0.1564, 0.1558, 0.1558], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 13400 | reward -0.6425000000000002 | loss 0.016110453953686932\n",
      "Q values: tensor([0.1866, 0.1980, 0.1954, 0.1855], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 13500 | reward -0.7892 | loss 0.029356518989420266\n",
      "Q values: tensor([0.2276, 0.2200, 0.2454, 0.2089], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 13600 | reward -1.7653999999999996 | loss 0.06327335762064834\n",
      "Q values: tensor([0.3197, 0.3292, 0.3199, 0.3030], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 34 | step 13685 | reward -1.4937499999999997 | loss 0.08099890622887429\n",
      "Q values: tensor([0.3826, 0.3785, 0.3639, 0.4002], grad_fn=<SelectBackward>)\n",
      " 14%|█▍        | 35/250 [13:46<1:29:17, 24.92s/it]\n",
      " Episode 35 | step 13700 | reward -0.06310000000000002 | loss 0.0011381891115691545\n",
      "Q values: tensor([0.1473, 0.1673, 0.1369, 0.1576], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 13800 | reward -1.0426 | loss 0.025168876490162972\n",
      "Q values: tensor([0.1932, 0.2037, 0.1945, 0.2067], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 13900 | reward -2.3599999999999994 | loss 0.07521198377370553\n",
      "Q values: tensor([0.2571, 0.2552, 0.2447, 0.2493], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 14000 | reward -3.212999999999998 | loss 0.15060213571957526\n",
      "Q values: tensor([0.5987, 0.5797, 0.6299, 0.5117], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 35 | step 14076 | reward -2.9937999999999985 | loss 0.6169709538965922\n",
      "Q values: tensor([0.5576, 0.5787, 0.5420, 0.5867], grad_fn=<SelectBackward>)\n",
      " 14%|█▍        | 36/250 [14:08<1:26:21, 24.21s/it]\n",
      " Episode 36 | step 14100 | reward 0.0 | loss 0.001356285061562268\n",
      "Q values: tensor([0.1522, 0.1573, 0.1579, 0.1642], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 14200 | reward -0.0643 | loss 0.00461967504509897\n",
      "Q values: tensor([0.1937, 0.1800, 0.1956, 0.1780], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 14300 | reward -0.6423499999999999 | loss 0.016478870541320245\n",
      "Q values: tensor([0.2490, 0.2461, 0.2537, 0.2543], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 14400 | reward -1.82405 | loss 0.047617690395975854\n",
      "Q values: tensor([0.3602, 0.3474, 0.3466, 0.3242], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 36 | step 14467 | reward -1.89905 | loss 0.08253665837383661\n",
      "Q values: tensor([0.3560, 0.3577, 0.3496, 0.3632], grad_fn=<SelectBackward>)\n",
      " 15%|█▍        | 37/250 [14:32<1:25:40, 24.13s/it]\n",
      " Episode 37 | step 14500 | reward -0.129 | loss 0.0017416474514542202\n",
      "Q values: tensor([0.1742, 0.1869, 0.1777, 0.1854], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 14600 | reward -0.8647000000000001 | loss 0.015810604987308463\n",
      "Q values: tensor([0.2316, 0.2306, 0.2146, 0.2204], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 14700 | reward -1.2786 | loss 0.03116682806827964\n",
      "Q values: tensor([0.2717, 0.2632, 0.2868, 0.2654], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 14800 | reward -1.7274999999999996 | loss 0.04767452653239712\n",
      "Q values: tensor([0.3672, 0.3889, 0.3915, 0.3954], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 37 | step 14858 | reward -2.1155999999999993 | loss 0.15034369258921043\n",
      "Q values: tensor([0.4355, 0.4486, 0.4459, 0.4557], grad_fn=<SelectBackward>)\n",
      " 15%|█▌        | 38/250 [14:57<1:25:51, 24.30s/it]\n",
      " Episode 38 | step 14900 | reward -0.09950000000000002 | loss 0.00338122910117189\n",
      "Q values: tensor([0.1657, 0.1640, 0.1720, 0.1751], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 15000 | reward -0.9207999999999997 | loss 0.016149760671339664\n",
      "Q values: tensor([0.2223, 0.2158, 0.2138, 0.2304], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 15100 | reward -1.5256999999999996 | loss 0.03501932877483682\n",
      "Q values: tensor([0.3377, 0.3221, 0.3453, 0.3583], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 15200 | reward -2.4475499999999997 | loss 0.05901979453554418\n",
      "Q values: tensor([0.4680, 0.4655, 0.4821, 0.4893], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 38 | step 15249 | reward -2.87875 | loss 0.24524759771579063\n",
      "Q values: tensor([0.5990, 0.5961, 0.6025, 0.5996], grad_fn=<SelectBackward>)\n",
      " 16%|█▌        | 39/250 [15:22<1:26:01, 24.46s/it]\n",
      " Episode 39 | step 15300 | reward -0.0345 | loss 0.00136534418280565\n",
      "Q values: tensor([0.1634, 0.1631, 0.1756, 0.1741], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 15400 | reward -0.11135000000000003 | loss 0.011723260988799389\n",
      "Q values: tensor([0.2013, 0.1875, 0.2189, 0.1981], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 15500 | reward -0.36100000000000004 | loss 0.04463729253648463\n",
      "Q values: tensor([0.3171, 0.2739, 0.2883, 0.2454], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 15600 | reward -3.3299499999999997 | loss 1.0218279495589604\n",
      "Q values: tensor([0.3525, 0.3253, 0.3498, 0.3556], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 39 | step 15640 | reward -3.4037999999999995 | loss 1.058686439618378\n",
      "Q values: tensor([0.4292, 0.3912, 0.4089, 0.4348], grad_fn=<SelectBackward>)\n",
      " 16%|█▌        | 40/250 [15:47<1:26:37, 24.75s/it]\n",
      " Episode 40 | step 15700 | reward -0.06750000000000002 | loss 0.00404262153868995\n",
      "Q values: tensor([0.1732, 0.1784, 0.1829, 0.1853], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 15800 | reward -0.7723000000000002 | loss 0.028869007710954797\n",
      "Q values: tensor([0.2225, 0.2198, 0.2237, 0.2222], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 15900 | reward -1.4630500000000002 | loss 0.06252377188169511\n",
      "Q values: tensor([0.2637, 0.2554, 0.2689, 0.2550], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 16000 | reward -1.8698 | loss 0.0734886511534878\n",
      "Q values: tensor([0.3274, 0.3287, 0.3377, 0.3207], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 40 | step 16031 | reward -2.05055 | loss 0.2007818691462817\n",
      "Q values: tensor([0.5331, 0.5244, 0.4647, 0.4961], grad_fn=<SelectBackward>)\n",
      " 16%|█▋        | 41/250 [16:13<1:27:07, 25.01s/it]\n",
      " Episode 41 | step 16100 | reward -0.40164999999999995 | loss 0.02694687675612073\n",
      "Q values: tensor([0.1804, 0.2082, 0.1753, 0.2212], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 16200 | reward -0.5381499999999997 | loss 0.03742477181968695\n",
      "Q values: tensor([0.2376, 0.2362, 0.2491, 0.2477], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 16300 | reward -0.83455 | loss 0.049786433954331244\n",
      "Q values: tensor([0.3845, 0.3678, 0.3713, 0.3591], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 16400 | reward -1.3565499999999995 | loss 0.061663802519667854\n",
      "Q values: tensor([0.4225, 0.4265, 0.4281, 0.4241], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 41 | step 16422 | reward -1.6075499999999994 | loss 0.17112913914184702\n",
      "Q values: tensor([0.4791, 0.4688, 0.4664, 0.4655], grad_fn=<SelectBackward>)\n",
      " 17%|█▋        | 42/250 [16:37<1:26:11, 24.86s/it]\n",
      " Episode 42 | step 16500 | reward -0.4910999999999999 | loss 0.008452467569302513\n",
      "Q values: tensor([0.2191, 0.2229, 0.2045, 0.2236], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 16600 | reward -0.8543999999999996 | loss 0.017869905197486435\n",
      "Q values: tensor([0.2612, 0.2582, 0.2684, 0.3062], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 16700 | reward -2.139699999999999 | loss 0.053364889641525526\n",
      "Q values: tensor([0.3236, 0.3381, 0.3333, 0.3457], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 16800 | reward -2.8296499999999987 | loss 0.06858368783401803\n",
      "Q values: tensor([0.4232, 0.4282, 0.4429, 0.4281], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 42 | step 16813 | reward -2.8091499999999985 | loss 0.12410475392404097\n",
      "Q values: tensor([0.4381, 0.4409, 0.4470, 0.4391], grad_fn=<SelectBackward>)\n",
      " 17%|█▋        | 43/250 [17:01<1:24:17, 24.43s/it]\n",
      " Episode 43 | step 16900 | reward -0.3785 | loss 0.010814120642605118\n",
      "Q values: tensor([0.1988, 0.2017, 0.1998, 0.2091], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 17000 | reward -0.927 | loss 0.02049721940851157\n",
      "Q values: tensor([0.2855, 0.2857, 0.2695, 0.2849], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 17100 | reward -1.7761999999999996 | loss 0.04694979218646722\n",
      "Q values: tensor([0.3417, 0.3338, 0.3459, 0.3364], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 17200 | reward -2.7883999999999984 | loss 0.07856925332183096\n",
      "Q values: tensor([0.3867, 0.3852, 0.3757, 0.3942], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 43 | step 17204 | reward -2.8483499999999986 | loss 0.15334304354840356\n",
      "Q values: tensor([0.3859, 0.3846, 0.3671, 0.3974], grad_fn=<SelectBackward>)\n",
      " 18%|█▊        | 44/250 [17:25<1:23:11, 24.23s/it]\n",
      " Episode 44 | step 17300 | reward -0.23450000000000007 | loss 0.00399285579358466\n",
      "Q values: tensor([0.1941, 0.2067, 0.2038, 0.2028], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 17400 | reward -0.72025 | loss 0.015552838895735555\n",
      "Q values: tensor([0.2476, 0.2453, 0.2527, 0.2515], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 17500 | reward -1.6636499999999994 | loss 0.0429724722942284\n",
      "Q values: tensor([0.3122, 0.3191, 0.2983, 0.3283], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 44 | step 17595 | reward -2.5864000000000003 | loss 0.13689130583052656\n",
      "Q values: tensor([0.3701, 0.3739, 0.3486, 0.3760], grad_fn=<SelectBackward>)\n",
      " 18%|█▊        | 45/250 [17:50<1:23:49, 24.53s/it]\n",
      " Episode 45 | step 17600 | reward 0.0 | loss 0.0003774135366896303\n",
      "Q values: tensor([0.1588, 0.1741, 0.1612, 0.1739], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 17700 | reward -0.31810000000000005 | loss 0.01893490989166935\n",
      "Q values: tensor([0.2241, 0.2321, 0.2382, 0.2449], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 17800 | reward -0.5997999999999999 | loss 0.02463245288930338\n",
      "Q values: tensor([0.3058, 0.3013, 0.2905, 0.2953], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 17900 | reward -3.080050000000003 | loss 0.21266353384705994\n",
      "Q values: tensor([0.3989, 0.3769, 0.3754, 0.3823], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 45 | step 17986 | reward -3.6678500000000054 | loss 0.32801132634990715\n",
      "Q values: tensor([0.4856, 0.4678, 0.4645, 0.4768], grad_fn=<SelectBackward>)\n",
      " 18%|█▊        | 46/250 [18:13<1:21:47, 24.06s/it]\n",
      " Episode 46 | step 18000 | reward -0.018500000000000003 | loss 0.0009626321277909078\n",
      "Q values: tensor([0.1636, 0.1776, 0.1779, 0.1814], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 18100 | reward -0.5690999999999999 | loss 0.011896209461383145\n",
      "Q values: tensor([0.2279, 0.2270, 0.2320, 0.2677], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 18200 | reward -2.0236 | loss 0.062451094594208834\n",
      "Q values: tensor([0.2827, 0.2842, 0.2862, 0.2928], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 18300 | reward -2.3423000000000003 | loss 0.07107843268419745\n",
      "Q values: tensor([0.3979, 0.3966, 0.3656, 0.3881], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 46 | step 18377 | reward -1.8069000000000006 | loss 0.5057343984057369\n",
      "Q values: tensor([0.4635, 0.4723, 0.4784, 0.4655], grad_fn=<SelectBackward>)\n",
      " 19%|█▉        | 47/250 [18:37<1:21:51, 24.20s/it]\n",
      " Episode 47 | step 18400 | reward -0.0095 | loss 0.0004759784015392343\n",
      "Q values: tensor([0.1736, 0.1798, 0.1717, 0.1729], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 18500 | reward -0.23345000000000002 | loss 0.005267513872183938\n",
      "Q values: tensor([0.2117, 0.2099, 0.2265, 0.2060], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 18600 | reward -1.6182 | loss 0.045554950008190986\n",
      "Q values: tensor([0.2655, 0.2680, 0.2746, 0.2564], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 18700 | reward -2.0556999999999985 | loss 0.056461643264004935\n",
      "Q values: tensor([0.3211, 0.3259, 0.3306, 0.3211], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 47 | step 18768 | reward -2.5113999999999987 | loss 0.12381621386611796\n",
      "Q values: tensor([0.3539, 0.3670, 0.3650, 0.3688], grad_fn=<SelectBackward>)\n",
      " 19%|█▉        | 48/250 [19:02<1:22:18, 24.45s/it]\n",
      " Episode 48 | step 18800 | reward -0.09500000000000001 | loss 0.0009391929530764287\n",
      "Q values: tensor([0.1845, 0.1907, 0.1728, 0.1810], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 18900 | reward -1.4850000000000003 | loss 0.034353680610387016\n",
      "Q values: tensor([0.2373, 0.2422, 0.2326, 0.2439], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 19000 | reward -2.7023999999999995 | loss 0.08265474096611314\n",
      "Q values: tensor([0.3192, 0.3032, 0.3121, 0.3061], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 19100 | reward -3.2339999999999987 | loss 0.11162875673071015\n",
      "Q values: tensor([0.5430, 0.5534, 0.5493, 0.5789], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 48 | step 19159 | reward -2.0398999999999985 | loss 0.47003497733192967\n",
      "Q values: tensor([0.6466, 0.6694, 0.6340, 0.6926], grad_fn=<SelectBackward>)\n",
      " 20%|█▉        | 49/250 [19:24<1:19:32, 23.74s/it]\n",
      " Episode 49 | step 19200 | reward -0.29700000000000004 | loss 0.004766222155574873\n",
      "Q values: tensor([0.1952, 0.1956, 0.1974, 0.1935], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 19300 | reward -1.2234500000000004 | loss 0.02176111277941839\n",
      "Q values: tensor([0.2562, 0.2526, 0.2498, 0.2606], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 19400 | reward -2.10775 | loss 0.041361411859173636\n",
      "Q values: tensor([0.4277, 0.4221, 0.4029, 0.4042], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 19500 | reward -3.5669 | loss 0.14965471994851942\n",
      "Q values: tensor([0.4990, 0.4806, 0.4920, 0.5106], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 49 | step 19550 | reward -2.3520999999999996 | loss 0.37120068373333215\n",
      "Q values: tensor([0.5903, 0.5812, 0.5622, 0.5813], grad_fn=<SelectBackward>)\n",
      " 20%|██        | 50/250 [19:49<1:20:19, 24.10s/it]\n",
      " Episode 50 | step 19600 | reward -0.07799999999999999 | loss 0.0008971111573785606\n",
      "Q values: tensor([0.1936, 0.1839, 0.1909, 0.1808], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 19700 | reward -0.7335 | loss 0.01503497611859217\n",
      "Q values: tensor([0.2487, 0.2329, 0.2463, 0.2320], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 19800 | reward -2.0240500000000003 | loss 0.05000216562395288\n",
      "Q values: tensor([0.3625, 0.3459, 0.3618, 0.3573], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 19900 | reward -2.2921 | loss 0.05585181309358589\n",
      "Q values: tensor([0.5724, 0.5305, 0.5015, 0.5053], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 50 | step 19941 | reward -3.2197000000000005 | loss 0.29822808864334505\n",
      "Q values: tensor([0.6316, 0.6129, 0.5884, 0.5949], grad_fn=<SelectBackward>)\n",
      " 20%|██        | 51/250 [20:13<1:19:48, 24.06s/it]\n",
      " Episode 51 | step 20000 | reward -0.06850000000000002 | loss 0.00034614439074420744\n",
      "Q values: tensor([0.1879, 0.1849, 0.1969, 0.1857], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 20100 | reward -1.2169999999999999 | loss 0.04234613681876764\n",
      "Q values: tensor([0.2376, 0.2423, 0.2400, 0.2429], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 20200 | reward -1.9119999999999984 | loss 0.05994166151875957\n",
      "Q values: tensor([0.3651, 0.3295, 0.3584, 0.3565], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 20300 | reward -2.312549999999998 | loss 0.06333285188168736\n",
      "Q values: tensor([0.5363, 0.4822, 0.5193, 0.5086], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 51 | step 20332 | reward -1.7019499999999976 | loss 0.07083988241675376\n",
      "Q values: tensor([0.5326, 0.4943, 0.5326, 0.5202], grad_fn=<SelectBackward>)\n",
      " 21%|██        | 52/250 [20:39<1:21:16, 24.63s/it]\n",
      " Episode 52 | step 20400 | reward -0.0897 | loss 0.000485122187492526\n",
      "Q values: tensor([0.1929, 0.1878, 0.1985, 0.1899], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 20500 | reward -0.5690999999999998 | loss 0.010102940562667806\n",
      "Q values: tensor([0.2326, 0.2236, 0.2193, 0.2229], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 20600 | reward -1.0726000000000002 | loss 0.018069014633807834\n",
      "Q values: tensor([0.2928, 0.2808, 0.2967, 0.2757], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 20700 | reward -2.2709 | loss 0.06478938974122017\n",
      "Q values: tensor([0.2921, 0.2907, 0.2779, 0.2905], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 52 | step 20723 | reward -1.9194999999999998 | loss 0.06730056652268801\n",
      "Q values: tensor([0.3490, 0.3570, 0.3666, 0.3388], grad_fn=<SelectBackward>)\n",
      " 21%|██        | 53/250 [21:05<1:21:37, 24.86s/it]\n",
      " Episode 53 | step 20800 | reward -0.029900000000000003 | loss 9.240984675340513e-05\n",
      "Q values: tensor([0.1881, 0.1974, 0.1946, 0.1912], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 20900 | reward -0.8844 | loss 0.026334542028893293\n",
      "Q values: tensor([0.2231, 0.2125, 0.2220, 0.2141], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 21000 | reward -1.8102000000000003 | loss 0.05685302342559073\n",
      "Q values: tensor([0.2429, 0.2404, 0.2364, 0.2434], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 21100 | reward -2.5703999999999985 | loss 0.07972927195752011\n",
      "Q values: tensor([0.4836, 0.4925, 0.4688, 0.4827], grad_fn=<SelectBackward>)\n",
      "\n",
      " Episode 53 | step 21114 | reward -2.7948999999999984 | loss 0.20474886804389503\n",
      "Q values: tensor([0.4901, 0.4711, 0.4890, 0.4903], grad_fn=<SelectBackward>)\n",
      " 22%|██▏       | 54/250 [21:29<1:20:39, 24.69s/it]"
     ]
    }
   ],
   "source": [
    "episodes = 250\n",
    "lr = 0.0001\n",
    "window_length = None\n",
    "eps = 1.0\n",
    "eps_decay = utils.linear_decay(epochs=20_000, start=eps, end=0.001)\n",
    "\n",
    "sigpolicy = SigPolicy(env, 5)\n",
    "sigpolicy.initialize_parameters(factor=1, zero_bias=True)\n",
    "\n",
    "results = train(env, \n",
    "                sigpolicy, \n",
    "                episodes,\n",
    "                discount=1.0,\n",
    "                learning_rate=lr, \n",
    "                exploration=\"softmax\",\n",
    "                epsilon=eps,\n",
    "                epsilon_decay=eps_decay,\n",
    "                window_length=window_length, \n",
    "                printing=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utils.plot_results([\n",
    "    results[\"rewards\"],\n",
    "    results[\"losses\"],\n",
    "    results[\"cash\"],\n",
    "    results[\"terminal_inventory\"],\n",
    "])\n",
    "\n",
    "id = -1\n",
    "\n",
    "observation_history = list(results[\"observations\"][id])\n",
    "plt.plot(observation_history)\n",
    "plt.xlabel(\"Observation history\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(results[\"actions\"][id])\n",
    "plt.xlabel(\"Action history\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(results[\"mid_prices\"][id])\n",
    "plt.xlabel(\"Mid-price history\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_reduced_100_runs_0804_1  # without inventory reward\n",
    "# results_reduced_50_runs_0804_2  # with inventory reward\n",
    "# results_reduced_50_runs_0804_3 # with inventory reward and correct epsilon decay\n",
    "# results_v2_200_runs_0808_1_convergence # inventory only model, with some convergence\n",
    "# results_v2_250_runs_0809_1 # inventory only model, with convergence\n",
    "# results_v2_250_runs_0809_2 # inventory only model, with convergence\n",
    "# results_v2_250_runs_0810_1 # inventory only model, with convergence\n",
    "# results_v2_250_runs_0811_1 # inventory only model, with no convergence\n",
    "# results_v2_250_runs_0811_2 # inventory only model, with convergence\n",
    "# results_v2_250_runs_0814_1 # inventory only model, with no convergence\n",
    "# results_v2_250_runs_0814_2 # inventory only model, with no convergence\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# write results \n",
    "with open('../results/results_NEW.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(results, f) # serialize the list\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test a policy\n",
    "\n",
    "#### Load training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_reduced_100_runs_0804_1  # without inventory reward\n",
    "# results_reduced_50_runs_0804_2  # with inventory reward\n",
    "# results_reduced_50_runs_0804_3 # with inventory reward and correct epsilon decay\n",
    "# results_v2_200_runs_0808_1_convergence # inventory only model, with some convergence\n",
    "# results_v2_250_runs_0809_1 # inventory only model, with some convergence\n",
    "# results_v2_250_runs_0809_2 # inventory only model, with some convergence\n",
    "# results_v2_250_runs_0810_1 # inventory only model, with some convergence\n",
    "# results_v2_250_runs_0811_1 # inventory only model, with no convergence\n",
    "# results_v2_250_runs_0811_2 # inventory only model, with convergence\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# load results\n",
    "with open('../results/results_v2_250_runs_0814_2.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# load the parameters from saved checkpoint (every 10 episodes)\n",
    "checkpoint = -1\n",
    "sigpolicy = SigPolicy(env, 5)\n",
    "sigpolicy.load_state_dict(results[\"intermediate\"][checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = 10\n",
    "env = generate_env()\n",
    "sigpolicy.eval()\n",
    "test_results = test(env, sigpolicy, runs, epsilon=0)\n",
    "sigpolicy.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_id = -1\n",
    "avg_window = 100\n",
    "names = [\"rewards\", \"cash\", \"actions\", \"inventories\"]\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, sharex=False)\n",
    "for ax, id in zip(axes.flat, range(4)):\n",
    "    ax.set_title(names[id] if id < 2 else names[id] + \" in episode \" + str(episode_id))\n",
    "    ax.plot(test_results[names[id]] if id < 2 else test_results[names[id]][episode_id])\n",
    "    ax.plot(utils.moving_average(\n",
    "            test_results[names[id]] if id < 2 else test_results[names[id]][episode_id], avg_window\n",
    "        ))\n",
    "    ax.set_xlabel(\"Episodes\" if id < 2 else \"Steps\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check first Q-value convergence\n",
    "\n",
    "**Note**: We need to compare this with the average episode reward once all runs are performed. Also longer runs might be needed since I expect the average episode reward to be above the Q values reached with 250 episodes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "policy = SigPolicy(env, 5)\n",
    "first_history = list(results[\"observations\"][0])[0:30]\n",
    "first_history = torch.tensor(\n",
    "    first_history, requires_grad=False, dtype=torch.float\n",
    ").unsqueeze(0)\n",
    "first_Q_values = []\n",
    "\n",
    "for state_dict in results[\"intermediate\"]: #range(len(results[\"intermediate\"])):\n",
    "    policy.eval()\n",
    "    policy.load_state_dict(state_dict)    \n",
    "    sig = policy.update_signature(first_history)  \n",
    "    first_Q_values.append(policy(sig).detach())       \n",
    "\n",
    "first_Q_values = torch.cat(first_Q_values, dim=0)\n",
    "plt.plot(first_Q_values[:,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "t = torch.tensor([0.0978, 0.0893, 0.1016, 0.1024])\n",
    "print(F.softmax(t, dim=-1))\n",
    "print(F.softmax(t/0.02, dim=-1))\n",
    "print(F.softmax(t/0.001, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
